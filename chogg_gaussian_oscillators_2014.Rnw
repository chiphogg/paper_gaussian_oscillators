\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath}
\usepackage{natbib}
\begin{document}
\title{Visualizing uncertain curves and surfaces via Gaussian Oscillators}
\author{Charles~R.~Hogg~III}
\date{August~2014}
\maketitle

<<global_setup, cache=FALSE, include=FALSE>>=
require(ggplot2)
require(knitr)
require(gridExtra)

source('interpolation.R')

# Smart caching.
opts_chunk$set(cache=TRUE, autodep=TRUE)

# ggplot2 options.
theme_set(theme_grey(base_size=12))
line.size <- 1

# For reproducibility.
set.seed(1)
@

\section{Introduction}
\label{sec:introduction}

Curves and surfaces abound in the sciences.
It is vital to quantify their uncertainty -- usually, by computing a probability distribution.
It is equally vital to visualize that uncertainty.
Here, curves and surfaces present considerable challenges.

The most common technique is to plot upper and lower credible intervals along with the estimated curve.
This shows considerable information at a glance.
However, as a pointwise technique, it discards correlations between points: for example, it cannot distinguish between slowly and rapidly varying curves.
Also, this technique is difficult to apply to surfaces, since they can easily obscure one another.

Showing individual draws from the distribution addresses the former problem nicely.
However, there is a fundamental tension: showing too few curves will not fully represent the distribution, while showing too many quickly clutters the figure.
What's more, the problem of obscured surfaces is only made more acute.

The solution is to show the draws one at a time.

((Elaborate.))

(Don't forget to mention how this connects to Gaussians!  GPs, and modes in parameter space.)

The present author is aware of two main approaches in the literature.

Historically, most animations have been based on \textit{interpolation}\cite{skilling1992,esg1997}.
A series of independent draws from the Gaussian are generated and displayed at regular time intervals; these are called the \textit{keyframes}.
The frames at intervening times are weighted averages of the nearest two keyframes.
The weighting favours the first keyframe at the beginning of the interval, and the second keyframe near the end, so that the curve or surface changes continuously from one keyframe to the next.
Figure \ref{fig:interpolation}(a) illustrates the idea for a simple 1-dimensional Gaussian using three possible interpolating families: \textit{linear}, \textit{cubic spline}, and \textit{trigonometric}.

<<interpolation_plots, include=FALSE>>=
quantiles <- c(0.1, 0.5, 0.9)
num.interpolated.curves <- 5e2  # Make it bigger later
num.points <- 500
num.keyframes <- 4
interpolation.caption <- sprintf(
    paste("Interpolation illustrated for a standard 1-D Gaussian.",
          "(a) A single set of normal draws interpolated by three methods.",
          "(b) Pointwise quantiles for interpolating %s random draws."),
    formatC(num.interpolated.curves, big.mark=',', format='fg'))

interp.matrix.functions <- list(Linear=InterpLinear,
                                Spline=InterpSpline,
                                Trigonometric=InterpTrig)

p.concept <- InterpolationConceptPlot(
    interp.matrix.functions=interp.matrix.functions,
    n.points=num.points,
    n.keyframes=num.keyframes,
    size=line.size)
p.interp <- InterpolationQuantilePlot(
    interp.matrix.functions=interp.matrix.functions,
    quantiles=quantiles,
    n.draws=num.interpolated.curves,
    n.points=num.points,
    n.keyframes=num.keyframes,
    size=line.size)
@

<<interpolation, cache=FALSE, echo=FALSE, fig.cap=interpolation.caption>>=
grid.arrange(p.concept + scale_colour_brewer('', palette='Set2'),
             p.interp + scale_colour_brewer('', palette='Set2'),
             ncol=1)
@

Unfortunately, interpolation can easily yield incorrect statistics between the keyframes.
In particular, the variance is usually underestimated.
Figure \ref{fig:interpolation}(b) shows that only the trigonometric approach correctly preserves statistical properties between keyframes.

((Say who did this.))

((Mention there are keyframes; they are bad.))

An important advance was made by Hennig\cite{hennig-tr-is-8}, who generated circular paths with marginal normal distributions.
A draw from the $m$-dimensional Gaussian formed one point on the circle.
A second independent draw set the direction of traversal.

Since no point on a circle is special, every frame in these circular timetraces is fully equivalent to every other.
To the best of the author's knowledge, Hennig's paper is the first published example of a smooth, keyframe-free Gaussian animation.

(Advantages and disadvantages of each.  Then say how mine is best of both worlds.)

\section{Gaussian Oscillators}
\label{sec:gaussian_oscillators}

(Explain the idea of a Gaussian Oscillator.)

(Explain they are a specific kind of time-domain Gaussian processes, but there may be more/less efficient ways to compute them.)

(Note: Hennig's great circles are not quite GOs, because the 1-D Gaussians are not independent; constrained to have constant mean.)

\section{A New Way to Generate Animations}
\label{sec:a_new_way_to_generate_animations}

The new animations in this paper are most naturally compared to the interpolation technique.
Both techniques involve generating some number $N$ of i.i.d. standard normal variates, then deterministically producing a continuous timetrace.
With interpolation, each variate influences only the small region between its nearest neighbours.
In this paper's technique, each variate's invluence is spread out across the entire timetrace.

Explicitly, given $N$ i.i.d. normal variates $\{\epsilon_1, \cdots, \epsilon_n\}$, the corresponding \textit{delocalized oscillator} $f(t)$ is given by
\begin{equation}
  \label{eqn:delocalized}
  f(t) = \sqrt{\frac{2}{N}} \sum\limits_{i=1}^{N/2} \left[
  \epsilon_{2i-1} \sin \left( \frac{2 \pi i t}{N} \right) +
  \epsilon_{2i} \cos \left( \frac{2 \pi i t}{N} \right)
\right].
\end{equation}
Note that this is only defined for even $N$, since every $\sin(\cdot)$ needs a corresponding $\cos(\cdot)$.
Considering the application, this restriction seems mild.

The formula \ref{eqn:delocalized} superficially resembles a Fourier series, in that it is a weighted sum of sines and cosines.

((Elaborate on differences))

(((more terms -> longer period, NOT finer motion)))

(((orthogonality: red herring! not really needed for different periods)))

(((coefficients also decrease with increasing $N$)))

To verify that $f(t)$ is a Gaussian Oscillator, the relations $\langle \epsilon_i \rangle = 0$ and $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$ will be useful.
First, note that $f(t)$ is a sum of Gaussian random variables at each time $t$; therefore, $f(t)$ also has a Gaussian distribution.
This means that its mean $\langle f(t) \rangle$ and covariance $\langle f(t_1) f(t_2) \rangle$ characterize it completely.

From the definition in Equation \ref{eqn:delocalized}, and the property $\langle \epsilon_i \rangle = 0$, it is clear that $\langle f(t) \rangle = 0$.
The covariance requires more work.
It involves a double sum, but since $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$, only the $i=j$ terms survive:
\begin{align}
  \langle f(t_1) f(t_2) \rangle &= \frac{2}{N} \sum\limits_{i=1}^{N/2}
    \left[
      \sin \left( \frac{2 \pi i t_1}{N} \right) \sin \left( \frac{2 \pi i t_2}{N} \right) +
      \cos \left( \frac{2 \pi i t_1}{N} \right) \cos \left( \frac{2 \pi i t_2}{N} \right)
    \right] \\
  &= \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
    \left( \frac{2 \pi i (t_2 - t_1)}{N} \right).
\end{align}
Note that this depends only on $(t_2 - t_1)$: these Gaussian Oscillators are \textit{stationary}, so no keyframes are singled out.

(Mention non-looping version)

\section{Comparison}
\label{sec:comparison}

\subsection{Basis functions}
\label{sub:basis_functions}

\subsection{Kinematic Properties}
\label{sub:kinematic_properties}

One way to compare Gaussian Oscillators is to compute their most basic kinematic properties: velocity and acceleration.
These distributions capture information which eludes the marginal \textit{position} distribution (which is the same by definition for all Gaussian Oscillators).

Figure ((make-a-figure!!)) shows the results.
The velocity distribution for interpolation changes discontinuously at every keyframe.
This makes the acceleration effectively infinite there.
At every keyframe, the oscillator receives a jarring ``impulse'' which distractingly changes its motion.

By contrast, the delocalized oscillators undergo perfectly smooth changes in motion.
In fact, their velocity and acceleration distributions are the same as the position distribution (up to a change of scale).

\subsection{Statistical Properties}
\label{sub:statistical_properties}

Gaussian oscillators are marginally Gaussian by definition.
However, this property applies to \textit{populations} of oscillators, not the individual oscillators themselves.

To see this point starkly, consider the simplest Gaussian Oscillator: a single Gaussian draw, constant for all time.
The marginal distribution over all such oscillators is obviously the standard Gaussian at all times.
But the marginal distribution for a \textit{single} oscillator is a point mass, which is very different from a Gaussian.

<<ks_data, include=FALSE>>=
# The number of Gaussian Oscillators of each type to construct.  Increasing
# this number gives more reliable density estimation.
num_oscillators <- 10000

# The number of equivalent independent normal draws to test.
equivalent_draws <- c(2, 10, 50)
names(equivalent_draws) <- equivalent_draws

# Utility function to compute the KS statistic on each column of a matrix.
KsOnColumns <- function(m) {
  apply(m, 2, function(v) ks.test(v, pnorm)$statistic)
}

RandomNormals <- function(n.rows, n.cols) {
  matrix(rnorm(n=n.rows * n.cols), nrow=n.rows)
}

# We want to create the desired number of copies of each oscillator.
# First, we'll do independent Gaussian draws as a reference point.
ks.independent <- lapply(equivalent_draws,
                         function(num) {
                           KsOnColumns(RandomNormals(num, num_oscillators))
                         })
# How many points to evaluate continuous timetraces at.  The higher this
# number, the more precision we have for *individual* oscillators' KS
# statistics.
num.continuous.points <- 1000
# Now we can compute the statistics for the Gaussian Oscillators (and a few
# more which aren't GOs).  Start by computing a matrix with enough normals for
# everything; we'll just take the top N rows to get N independent draws.
common.seed <- RandomNormals(max(equivalent_draws), num_oscillators)
KsForMethod <- function(matrix.function) {
  lapply(equivalent_draws,
         function(num) {
           KsOnColumns(
               matrix.function(
                   num, TimeSequence(num, num.continuous.points, loop=TRUE))
               %*% common.seed[1:num, ])
         })
}
ks.trig <- KsForMethod(InterpTrig)
ks.delocalized <- KsForMethod(DelocalizedMatrix)

ListToDataFrame <- function(L, name) {
  d <- cbind(melt(do.call(cbind.data.frame, L)), type=name)
  names(d)[which(names(d) == 'variable')] <- 'num.draws'
  names(d)[which(names(d) == 'value')] <- 'x'
  return (d)
}
ks.all <- rbind(
    ListToDataFrame(ks.independent, 'Independent Draws'),
    ListToDataFrame(ks.trig, 'Trigonometric Interpolation'),
    ListToDataFrame(ks.delocalized, 'Delocalized (this work)'))
ks.figure.caption <- sprintf(
    paste('How faithfully does each Gaussian Oscillator represent the normal',
          'distribution? This figure uses the Kolmogorov-Smirnov distribution',
          'to answer that question.  (Each curve is a histogram of %s draws.)',
          '(a) Independent draws from the normal distribution (which are not',
          'Gaussian Oscillators) provide a basis for comparison.  As expected,',
          'taking more draws shifts the distribution towards smaller values,',
          'indicating a better fit.',
          '(b), (c), and (d) show the Gaussian Oscillators for %d, %d, and %d',
          'draws, respectively, alongside the same number of independent',
          'draws.  Trigonometric interpolation is always more faithful than',
          'the same number of independent draws, but the delocalized',
          'oscillators this paper introduces are significantly better than',
          'both.'),
    formatC(num_oscillators, big.mark=',', format='fg'),
    equivalent_draws[1],
    equivalent_draws[2],
    equivalent_draws[3])
@

This motivates us to evaluate a family of Gaussian Oscillators by how faithfully its members represent the standard Gaussian distribution.
The Kolmogorov-Smirnov (KS) statistic -- essentially, the widest vertical gap between two CDFs -- is a convenient, widely-used technique to compare two distributions.
The KS \textit{distribution} summarizes the fidelity of a Gaussian Oscillator \textit{family}: distributions which place more mass on smaller values indicate more faithful families.

As a basis for comparison, consider the KS distribution for a given number $N$ of independent normal draws, without interpolation.
Larger samples represent their distributions more faithfully; therefore, we expect their KS distributions to shift towards zero.
Figure \ref{fig:ks_distribution}(a) shows the results for \Sexpr{equivalent_draws[1]}, \Sexpr{equivalent_draws[2]}, and \Sexpr{equivalent_draws[3]} draws.
It is clear that larger samples indeed have smaller KS statistics.

The next three subfigures compare two types of Gaussian Oscillator\footnote{
  We cannot include the Great Circles approach of \citep{hennig-tr-is-8}, since technically they are not Gaussian Oscillators.
  Individual dimensions are not independent; they are constrained so that the overall magnitude stays constant.
  However, each dimension is \textit{marginally} equivalent to the $N=2$ delocalized oscillators.
  Thus, the purple curve in Figure \ref{fig:ks_distribution}(b) can be taken to apply to Hennig's work, but the subsequent purple curves cannot (since Great Circles are restricted to $N=2$).
} -- trigonometric interpolation, and the delocalized oscillators this paper introduces -- to the equivalent number of independent draws.
The figure shows that interpolating between the independent draws (orange curve) more faithfully represents the normal distribution than the independent draws themselves (green curve), since the orange curve puts more mass on lower KS values.
However, the delocalized oscillators (purple curve) are significantly better than either in every case shown here.

Thus, not only is their motion more smooth and natural, but they represent the underlying distribution more faithfully.

<<ks_distribution, echo=FALSE, cache=FALSE, fig.cap=ks.figure.caption>>=
KsFigure <- function(i) {
  d <- ks.all[i, ]
  p <- (ggplot(data=d, aes(x=x, y=..density.., colour=type,
                           group=interaction(type, num.draws)))
        + geom_freqpoly(binwidth=diff(range(d$x))/60, size=line.size)
        + theme(legend.position='none')
        + scale_colour_brewer('', palette='Set2')
        + scale_x_continuous('Kolmogorov-Smirnov statistic')
        + scale_y_continuous('Density')
        )
}
independent.peaks <- data.frame(draws=equivalent_draws,
                                ks=c(0.5, 0.25, 0.125),
                                density=c(3.7, 5.5, 11.5))
colour.legend <- theme(legend.justification=c(1, 1),
                       legend.position=c(1, 1),
                       legend.margin=unit(0, 'mm'),
                       legend.title=element_blank())
grid.arrange(ncol=2,
             KsFigure(which(ks.all$type == 'Independent Draws'))
                 + geom_text(inherit.aes=FALSE, hjust=0, data=independent.peaks,
                             aes(label=draws, x=ks, y=density))
                 + ggtitle('(a) Independent draws'),
             KsFigure(which(ks.all$num.draws == 2))
                 + colour.legend
                 + ggtitle(sprintf('(b) %d draws', equivalent_draws[1])),
             KsFigure(which(ks.all$num.draws == 10))
                 + colour.legend
                 + ggtitle(sprintf('(c) %d draws', equivalent_draws[2])),
             KsFigure(which(ks.all$num.draws == 50))
                 + colour.legend
                 + ggtitle(sprintf('(d) %d draws', equivalent_draws[3])))
@

The distributions in Figure \ref{fig:ks_distribution}(b) have several interesting features.
To understand this advantage, consider the $N=2$ case in Figure \ref{fig:ks_distribution}(b).

Both the independent draws and the trigonometric interpolation place 50\% of their total mass on KS values above 0.5.
This is because two random normal draws have the same sign with probability 0.5.
When this happens, interpolation cannot generate values of the opposite sign, but these account for half the total mass of the standard normal.

By contrast, the delocalized oscillators can never yield KS values above 0.5.
This is because the linear combination of sine and cosine is equivalent to a sine curve with a random amplitude and phase (and the latter does not matter when we average over a whole period).
Hence, both positive and negative values are always included\footnote{The only exception is the delocalized oscillator which is identically zero at all times, but this happens with probability 0.}.

In short: the probability for the KS value to be worse than 0.5 is 0 for delocalized oscillators, and 0.5 for the alternatives.
This helps explain the considerable advantage for the former.

Delocalized oscillators retain this advantage for higher values of $N$, although the reasons why are less clear.
In any case, the basic story remains the same: interpolating between independent draws brings a marginal increase in fidelity, while delocalizing them brings a significant increase.

\section{Future Work}
\label{sec:future_work}

\subsection{Animating non-Gaussian distributions}
\label{sub:animating_non_gaussian_distributions}

This paper has shown how to visualize arbitrary Gaussian distributions using continuous animations.
The question naturally arises which other distributions can be visualized in this way.
Some distributions are clearly impossible, but several others seem tractable.

First, consider a ``nearly-Gaussian'' distribution, i.e., one whose probability density $\rho(x)$ differs from the Gaussian density $\nu(x)$ by some known factor (which naturally depends on $x$).
The time which a standard Gaussian Oscillator spends at $x$ is proportional to $\nu(x)$.
To make it proportional to $\rho(x)$ instead, we can modulate the delay before the next frame by a factor $(\rho(x) / \nu(x))$.
In essence, we are warping time to capture the subtle departures of the true distribution from the Gaussian.

A more challenging case is distributions with multiple regions that are connected only thinly (or even disconnected entirely).
Time warping seems infeasible here for several reasons.
First, the oscillator would need to move infinitely quickly in the intermodal region.
Furthermore, any single Gaussian is likely to be a very poor approximation to the distribution as a whole.

A better alternative would be to show a separate animation for each region.
A Gaussian can better approximate individual modes rather than the entire distribution.
And the total probability mass of each mode can be indicated beside its animation.

Discrete distributions present a greater challenge: obviously, they cannot be represented by continuous animations.
However, even here there are special cases which admit partial success.
Consider a Poisson distribution (representing, e.g., neutron counts in a scattering curve\cite{hklm2010}).
The Anscombe transform\cite{anscombe1948} takes Poisson-distributed data into Gaussian-distributed to an excellent approximation.
One could animate the transformed distribution with Gaussian Oscillators, ``un-transform'' the result, and round to the nearest integer.
While the result would not be truly continuous, it would change in an ordered way which is easy for the eye to follow.

Animations are highly useful for visualizing complex distributions.
With the Gaussian case now well in hand, these other distributions beg to be explored.

\subsection{Alternative time-domain covariance functions}
\label{sub:alternative_time_domain_covariance_functions}

(Compact support: endless exploration!)

(Challenging to do in R: no framebuf support.)

((However, could output javascript with d3.js!))

\section{Conclusions}
\label{sec:conclusions}

\bibliographystyle{plain}
\bibliography{biblio}
\end{document}

Ideas which need a home

(Looping vs. non-looping)
