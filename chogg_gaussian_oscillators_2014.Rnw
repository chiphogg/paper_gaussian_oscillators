\documentclass[12pt]{article}
\usepackage{amsmath,amssymb}
\usepackage{natbib}
\usepackage{url}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\DeclareMathOperator{\sinc}{sinc}
\newcommand{\transpose}{\intercal}
\newcommand{\iid}{i.i.d.}
\newcommand{\order}{\mathcal{O}}

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


\title{Visualizing uncertain curves and surfaces via Gaussian Oscillators}
\author{Charles~R.~Hogg~III}
\date{January~2015}
\maketitle

<<global_setup, cache=FALSE, include=FALSE>>=
require(ggplot2)
require(knitr)
require(gridExtra)

source('interpolation.R')

# "Draft mode" makes sketch-y figures with low statistics.
draft_mode <- TRUE

# Smart caching.
opts_chunk$set(cache=TRUE, autodep=TRUE)

# ggplot2 options.
theme_set(theme_grey(base_size=11))
line.size <- 1

# For reproducibility.
set.seed(1)

# Extract the legend grob from a graph.
# http://stackoverflow.com/a/12539820
LegendOnly <- function(a.gplot) {
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == 'guide-box')
  tmp$grobs[[leg]]
}
@

\begin{abstract}
  We study animations as tools to visualize Gaussian uncertainty in curves and surfaces.
  First, we define the \textit{Gaussian oscillator}, a unifying concept which encompasses existing and future approaches.
  We then develop a new technique, the \textit{delocalized oscillator}, which combines the best traits of previous ones: these animations feature fluid, natural motion, and also better represent the underlying distribution.
  We propose the first metric to judge animation techniques by the quality of individual animations (rather than the entire population): the Kolmogorov-Smirnov distribution.
  Finally, we suggest how to develop even better animations in the future by leveraging a key insight: that Gaussian animations are nothing more than time-domain Gaussian processes.
\end{abstract}

\noindent%
{\it Keywords:}  Gaussian Processes; animations; Bayesian
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!
\section{Introduction}
\label{sec:introduction}

Curves and surfaces abound in the sciences.
It is vital to quantify their uncertainty --- say, by computing a probability distribution.
It is equally vital to visualize that uncertainty.
Here, curves and surfaces present considerable challenges.

The most common technique is to plot upper and lower credible intervals along with the estimated curve.
This useful technique conveys considerable information at a glance, but there is some information it cannot convey.
Since intervals are pointwise, they tell us nothing about correlations between points.
In other words, we know the curve lies roughly within a boundary, but we don't know what it's doing within that boundary.

To convey this missing information, we need a complementary technique which depicts individual members of the distribution.
The simplest such approach is to sample several curves from the distribution, and plot them together.
Unfortunately, there is a fundamental tension: showing too few curves will not fully represent the distribution, while showing too many quickly clutters the figure.

Animations resolve this tension by introducing a time dimension: we show as many curves as necessary, but only one at a time.
If each frame is correlated with its neighbours, the animation becomes \textit{continuous}, and the curve appears as a quasi-physical moving object.
The eye is naturally drawn to areas with more motion, which represent higher uncertainty.
Continuous animations are thus an intuitive and interpretable way to visualize uncertainty in curves.

Naturally, all these considerations apply to uncertain \textit{surfaces} too, but even more strongly.
Credible intervals for surfaces are difficult to visualize because the surfaces obscure one another.
Animations avoid this difficulty, because they never show more than one surface at a time.

This paper focuses on curves and surfaces whose underlying distribution is Gaussian.
This is not nearly as restrictive as it might seem.
It includes Gaussian processes \citep{3569}, a flexible nonparametric class of models.
Furthermore, parametric models often have a single posterior mode, which is Gaussian to leading order.
Continuous paths through the Gaussian parameter space correspond to continuous animations of the function those parameters govern.

The literature contains two main approaches so far.

Historically, most animations have been based on \textit{interpolation} \citep{skilling1992,esg1997}.
A series of independent draws from the Gaussian are generated and displayed at regular time intervals; these are called the \textit{keyframes}.
The frames at intervening times are weighted averages of the nearest two keyframes.
The weighting favours the first keyframe at the beginning of the interval, and the second keyframe near the end, so that the curve or surface changes continuously from one keyframe to the next.
Figure \ref{fig:interpolation}(a) illustrates the idea for a simple 1-dimensional Gaussian using three possible interpolating families: \textit{linear}, \textit{cubic spline}, and \textit{trigonometric}.

<<interpolation_plots, include=FALSE>>=
quantiles <- c(0.1, 0.5, 0.9)
num.interpolated.curves <- ifelse(draft_mode, 1e3, 1e6)
num.points <- 500
num.keyframes <- 4
interpolation.caption <- sprintf(
    paste("Interpolation illustrated for a standard 1-D Gaussian.",
          "(a) A single set of normal draws interpolated by three methods.",
          "It is not obvious from this figure which of these methods, if any, is best.",
          "(b) Pointwise quantiles (%s) for interpolating %s sets of random draws.",
          "These show that linear and spline interpolation are unacceptable, because the population statistics are different between the keyframes.",
          "Only trigonometric interpolation yields the same marginal distribution at all times."),
    paste(collapse=", ", c(head(quantiles, -1), paste('and', tail(quantiles, 1)))),
    formatC(num.interpolated.curves, big.mark=',', format='fg'))

interp.matrix.functions <- list(Linear=InterpLinear,
                                Spline=InterpSpline,
                                Trigonometric=InterpTrig)

p.concept <- (
    InterpolationConceptPlot(
        interp.matrix.functions=interp.matrix.functions,
        n.points=num.points,
        n.keyframes=num.keyframes,
        size=line.size)
    + scale_x_continuous('Time')
    + scale_y_continuous('Value')
    )
p.interp <- (
    InterpolationQuantilePlot(
        interp.matrix.functions=interp.matrix.functions,
        quantiles=quantiles,
        n.draws=num.interpolated.curves,
        n.points=num.points,
        n.keyframes=num.keyframes,
        size=line.size)
    + scale_x_continuous('Time')
    + scale_y_continuous('Value')
    )
@

<<interpolation, echo=FALSE, fig.cap=interpolation.caption>>=
grid.arrange(p.concept + scale_colour_brewer('', palette='Set2'),
             p.interp + scale_colour_brewer('', palette='Set2'),
             ncol=1)
@

Unfortunately, interpolation can easily yield incorrect statistics between the keyframes.
In particular, the variance is usually underestimated.
Figure \ref{fig:interpolation}(b) shows that only the trigonometric approach \citep{esg1997,skilling1992} correctly preserves statistical properties between keyframes.

A creative alternative was pioneered by Hennig \citep{hennig-tr-is-8}, who generated marginally normal great-circle orbits through parameter space.
The starting point is a random draw from the multivariate Gaussian.
Motion begins in the direction of a second random draw, and is constrained so that the particle's distance from the origin is constant.
Since no point on a circle is special, every frame in these circular timetraces is fully equivalent to every other.
To the best of this author's knowledge, Hennig's paper is the first published example of a smooth, keyframe-free Gaussian animation.

Each approach has advantages and weaknesses.
Interpolation is flexible: it can incorporate arbitrarily many independent draws, yielding paths which better represent the Normal distribution.
The downside is that not all frames are equivalent: keyframes have preferred status, and the motion changes discontinuously there, which is distracting.

Great circle animations \citep{hennig-tr-is-8} do not have this defect; they treat all frames equally in every way.
They also represent the normal distribution more faithfully than interpolated animations for the same number of samples, as Section \ref{sub:statistical_properties} will elaborate.
Their shortcoming is that they cannot incorporate more than two samples; therefore, they can only visualize a small part of the distribution.

This paper proposes a unifying concept which encompasses these and future approaches: the \textit{Gaussian oscillator}.
We then introduce a novel Gaussian oscillator technique, which combines the extensibility of interpolation with the smooth, natural motion of the great circles.
We develop quality metrics to assess the strengths and weaknesses of each method.
Finally, we suggest how to leverage the Gaussian oscillator framework to construct even better animations in the future.

\section{Gaussian oscillators}
\label{sec:gaussian_oscillators}

This section will define and explain the \textit{Gaussian oscillator}: the key concept for analyzing Gaussian animations.

First, consider an $m$-dimensional Gaussian, with mean vector $\mu$ and covariance matrix $K$.
To draw a sample from this distribution, we need the lower-triangular Cholesky decomposition $L$ of $K$, which satisfies $LL^\intercal = K$.
If $\xi \equiv \left( \xi_1, \cdots, \xi_m \right)^\intercal$ represents $m$ \iid~draws from the \textit{standard} normal $N(0, 1)$, then $\alpha \equiv \mu + L\xi$ is a draw from $N(\mu, K)$.

Now imagine that each $\xi_i$ is a random function of time, $\xi_i(t)$, such that $\xi_i(t) \sim N(0, 1) \,\, \forall t$ (marginally).
It follows that $\alpha(t) \sim N(\mu, K) \,\, \forall t$ (again, marginally).
Thus, the problem of animating arbitrary multidimensional Gaussian distributions reduces to that of animating a single-dimensional standard Gaussian.
This latter problem is the focus of the rest of the paper.

This motivates us to define a \textit{Gaussian oscillator} as a stochastic process, $\xi(t)$, with the following properties.
\begin{equation}
  \xi(t) \sim N(0, 1) \,\, \forall t
  \label{eqn:go_marginal}
\end{equation}
\begin{equation}
  \lim_{\Delta t \rightarrow 0} \langle \xi(t) \xi(t + \Delta t) \rangle = 1 \,\, \forall t
  \label{eqn:go_continuous}
\end{equation}
Equation \ref{eqn:go_marginal} means that every frame of the animation is marginally Gaussian.
Equation \ref{eqn:go_continuous} means that the animation is \textit{continuous}.

Most $m$-dimensional Gaussian animations can be analyzed by combining $m$ independent Gaussian oscillators.
Great circle animations are an exception, since they sample directly from the $m$-dimensional distribution.
Each individual dimension of a great circle animation fulfills the definition of a Gaussian oscillator.
However, the oscillators are not independent, since the $m$-dimensional vector is constrained to have constant magnitude.

\subsection{Connection with Gaussian processes}
\label{sub:connection_with_gaussian_processes}

Readers familiar with the concept may have noticed that Gaussian oscillators are also Gaussian \textit{processes} \citep{3569}, with a few constraints.
Specifically, the mean function $\mu(t)$ is zero everywhere, and the covariance function $k(t, t')$ is 1 when $t = t'$ (and continuous in that neighbourhood).

Remarkably, nobody seems to have pointed out this connection before.
It is extremely useful, since future Gaussian animation research can leverage the well-studied menagerie of covariance functions \citep{3569}.

There is some danger of confusion here, because often the curves and surfaces being animated are also modeled using Gaussian processes.
The difference is that the Gaussian oscillator is a \textit{time-}domain Gaussian process, while the curve or surface lives in the \textit{space} domain\footnote{
The ``space'' here might be an abstract space.  It might even be (physical) time --- for example, with time-series analysis.  However, the point is that this is distinct from the ``animation time'' domain where the Gaussian oscillator lives.
}.

Treating Gaussian oscillators as Gaussian processes is likely to be more useful for theoretical analysis than for practical computations\footnote{
  Gaussian processes with compact support are a notable exception, as discussed in the appendix.
}.
Consider a trigonometric interpolation of $k$ keyframes and $n$ total frames; typically, $n \gg k$.
Computing $\xi(t)$ as a Gaussian process requires $n$ \iid~normal draws, and an $\order(n^3)$ matrix inversion!
By contrast, explicit interpolation requires only $k$ normal draws (and simple sines and cosines instead of the matrix inversion).

\subsection{Offline and online animations}
\label{sub:offline_and_online_animations}

It is useful to distinguish between ``offline'' and ``online'' animations.
An ``offline'' animation is one which is fully constructed before it is viewed.
The classic example is an animated \texttt{.gif}.
For ``online'' animations, later frames are constructed while earlier frames are being viewed.
A modern example would be any javascript plotting library which uses \texttt{CSS3} transitions.

Note that the online/offline distinction has nothing to do with internet connectivity.
For example, an animated \texttt{.gif} is the archetype of an ``offline'' animation, but it is usually shared over the internet.
Moreover, ``online'' animations based on javascript and CSS do not require an internet connection, since the computation happens on the client side.

Offline animations suffer from a fundamental tension.
With too few frames, they represent their underlying distribution poorly (as Section \ref{sub:statistical_properties} will elaborate).
With too many frames, the file size becomes impractically huge.
For this reason, online animations are preferable when available.

\section{A New Way to Generate Animations}
\label{sec:a_new_way_to_generate_animations}

We introduce our new animation technique by comparing it to interpolation.
Both techniques involve generating some number $N$ of \iid~standard normal variates, then using them deterministically to produce a continuous timetrace.\footnote{
  These random variates can be viewed as coefficients for a set of basis functions, as Figure \ref{fig:basis} shows.
}
With interpolation, each variate influences only the local region between its nearest neighbours.
In this paper's technique, each variate's influence is spread out across the entire timetrace.

<<common_prep, include=FALSE>>=
# Parameters to govern the output for the basis and kinematics plots.
N.out <- 400
N.frames <- 4
N.draws <- 20
dt <- N.frames / N.out
t.out <- seq(from=dt, to=N.frames, length=N.out)

weight <- function(t.in, t.out, FUN) {
  ifelse(abs(t.in - t.out) > 1, 0, FUN(t.in, t.out))
}
periodic_weight <- function(t.in, t.out, period, FUN) {
  # Hack to make the basis functions periodic (instead of always 0 at t=0).
  pmax(weight(t.in, t.out, FUN), weight(t.in, t.out - period, FUN))
}

DelocalizedMatrix <- function(N.frames, t) {
  N <- N.frames / 2
  cbind(
    outer(t, 1:N, function(t, i) sin(pi * i * t / N)),
    outer(t, N:1, function(t, i) cos(pi * i * t / N))) / sqrt(N)
}
InterpolationMatrixESG <- function(N.frames, t.out) {
  t.frames <- 1:N.frames
  outer(t.out, t.frames, 
    function(x, y) {
      periodic_weight(x, y, N.frames, function(a, b) cos((a - b) * pi / 2))
    })
}

# Matrices.
interp.esg <- InterpolationMatrixESG(N.frames, t.out)
interp.deloc <- DelocalizedMatrix(N.frames, t.out)
@

<<basis_prep, include=FALSE>>=
basis.colors <- scale_colour_brewer(palette='Set2')
basis.theme <- theme(legend.position='none')
basis.plot <- function(mat, title) {
  d <- data.frame(y=as.vector(mat), t=t.out,
    label=rep(paste0('a', 1:N.frames), each=N.out))
  p <- (ggplot(data=d, aes(x=t, y=y, colour=label))
    + geom_hline(yintercept=0)
    + geom_line(size=1)
    + scale_x_continuous('Time', limits=c(0, N.frames))
    + scale_y_continuous('Value', limits=c(-1, 1), breaks=(-1):1)
    + basis.colors
    + basis.theme
    + ggtitle(title)
    )
  return (p)
}
basis.caption <- sprintf(
  paste(
    'Basis functions for two types of Gaussian oscillators (each based on %d independent draws, and periodic with period %d).',
    'a) Trigonometric interpolation basis functions localize the influence of each sample, but the slope is discontinuous at the boundaries.',
    'b) Delocalized oscillator basis functions spread the influence of each sample across the entire timetrace, and are perfectly smooth everywhere.'),
  N.frames,
  N.frames
  )
@

<<basis, echo=FALSE, fig.cap=basis.caption, fig.height=4>>=
grid.arrange(ncol=1,
  basis.plot(interp.esg, 'a) Trigonometric basis functions'),
  basis.plot(interp.deloc, 'b) Delocalized basis functions'))
@

Explicitly, given $N$ \iid~normal variates $\{\epsilon_1, \cdots, \epsilon_n\}$, the corresponding \textit{delocalized oscillator} $\xi(t)$ is given by
\begin{equation}
  \label{eqn:delocalized}
  \xi(t) = \sqrt{\frac{2}{N}} \sum\limits_{i=1}^{N/2} \left[
  \epsilon_{2i-1} \sin \left( \frac{2 \pi i t}{N} \right) +
  \epsilon_{2i} \cos \left( \frac{2 \pi i t}{N} \right)
\right].
\end{equation}
$\xi(t)$ is an offline technique.
It produces periodic timetraces, with period $N$.
Note that $\xi(t)$ is only defined for even $N$, since every $\sin(\cdot)$ needs a corresponding $\cos(\cdot)$.
Considering the application, this restriction seems mild.

To verify that $\xi(t)$ is a Gaussian oscillator, the relations $\langle \epsilon_i \rangle = 0$ and $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$\footnote{
  $\delta_{ij}$ is the Kronecker delta symbol, which is 1 when $i = j$ and 0 otherwise.
} will be useful.
First, note that $\xi(t)$ is a sum of Gaussian random variables at each time $t$; therefore, $\xi(t)$ also has a Gaussian distribution.
This means that its mean $\langle \xi(t) \rangle$ and covariance $\langle \xi(t_1) \xi(t_2) \rangle$ characterize it completely.

From the definition in Equation \ref{eqn:delocalized}, and the property $\langle \epsilon_i \rangle = 0$, it is clear that $\langle \xi(t) \rangle = 0$.
The covariance requires more work.
It involves a double sum, but since $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$, only the $i=j$ terms survive:
\begin{align}
  \langle \xi(t_1) \xi(t_2) \rangle &= \frac{2}{N} \sum\limits_{i=1}^{N/2}
    \left[
      \sin \left( \frac{2 \pi i t_1}{N} \right) \sin \left( \frac{2 \pi i t_2}{N} \right) +
      \cos \left( \frac{2 \pi i t_1}{N} \right) \cos \left( \frac{2 \pi i t_2}{N} \right)
    \right] \\
  &= \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
    \left( \frac{2 \pi i (t_2 - t_1)}{N} \right).
    \label{eqn:do_covariance}
\end{align}
Note that this depends only on $(t_2 - t_1)$: these Gaussian oscillators are \textit{stationary}, so no keyframes are singled out.
Note too that $\langle \xi(t)^2 \rangle = 1 \,\,\,\, \forall t$, as required for Gaussian oscillators.

\section{Comparison}
\label{sec:comparison}

This section introduces criteria to judge the quality of each type of Gaussian oscillator.
We will assume throughout that the animation is composed of independent Gaussian oscillators, which is usually the case.
Great circle animations \citep{hennig-tr-is-8} violate this assumption, so we must begin by clarifying their role.

Great circle animations are very similar to the $N = 2$ case of delocalized oscillators ($\text{DO}_2$ for short).
Both techniques require $2d$ random variables to visualize a $d$-dimensional Gaussian, and both techniques produce orbits which are confined to a (randomly-oriented) plane.
In fact, great circle animation is a subset of measure zero of the $\text{DO}_2$ family: the former always produces circles, but the latter can also produce ellipses.
Therefore, we assume that our results for $\text{DO}_2$ animations are a useful heuristic for the quality of the great circle animations.\footnote{
  Basically, this assumption amounts to ignoring the fact that the dimensions of a great circle animation are not independent.
}

\subsection{Statistical Properties}
\label{sub:statistical_properties}

How well does an animation technique represent the Gaussian distribution?
This question has two possible meanings, but previous literature has only considered one.

So far, people have taken this question to refer to a \textit{family} of animations.
Specifically, each family's distribution at each individual time is marginally Gaussian.
This interpretation is certainly important, which is why we built it in to the definition of the Gaussian oscillator (Equation \ref{eqn:go_marginal}).

However, the properties of \textit{individual} animations are even more important.
(After all, a typical visualization contains only a single animation, not a family of them.)
The frames of an individual animation define a probability distribution.
The closer this distribution approximates the standard Gaussian, the better the technique.

To illustrate the distinction between family and individual, consider the simplest Gaussian oscillator: a single Gaussian draw, constant for all time.
The marginal distribution over this family is clearly the standard Gaussian at all times.
But the marginal distribution for a \textit{single} animation is a point mass, which is very different from a Gaussian.

<<ks_data, include=FALSE>>=
# The number of Gaussian oscillators of each type to construct.  Increasing
# this number gives more reliable density estimation.
num_reps <- ifelse(draft_mode, 1, 10)
num_per_rep <- ifelse(draft_mode, 0.5e3, 1e5)
num_oscillators <- num_reps * num_per_rep

# The number of equivalent independent normal draws to test.
equivalent_draws <- c(2, 10, 50)
names(equivalent_draws) <- equivalent_draws

# Utility function to compute the KS statistic on each column of a matrix.
KsOnColumns <- function(m) {
  apply(m, 2, function(v) ks.test(v, pnorm)$statistic)
}

RandomNormals <- function(n.rows, n.cols) {
  matrix(rnorm(n=n.rows * n.cols), nrow=n.rows)
}

# We want to create the desired number of copies of each oscillator.
# First, we'll do independent Gaussian draws as a reference point.
for (i in 1:num_reps) {
  new_draws <- lapply(equivalent_draws,
                      function(num) {
                        KsOnColumns(RandomNormals(num, num_per_rep))
                      })
  if (!draft_mode) {
    system(sprintf('touch progress_indep_%d', i))
  }
  if (exists('ks.independent') && length(ks.independent) > 0) {
    ks.independent <- mapply(c, ks.independent, new_draws, SIMPLIFY=FALSE)
  } else {
    ks.independent <- new_draws
  }
}
# How many points to evaluate continuous timetraces at.  The higher this
# number, the more precision we have for *individual* oscillators' KS
# statistics.
num.continuous.points <- 1000
# Now we can compute the statistics for the Gaussian oscillators (and a few
# more which aren't GOs).  Start by computing a matrix with enough normals for
# everything; we'll just take the top N rows to get N independent draws.
KsForMethod <- function(matrix.function, label='none') {
  FUN <- function(num) {
    KsOnColumns(matrix.function(num,
                                TimeSequence(num,
                                             num.continuous.points,
                                             loop=TRUE))
                %*% RandomNormals(num, num_per_rep))
  }
  remove('ks')
  for (i in 1:num_reps) {
    new_draws <- lapply(equivalent_draws, FUN)
    if (!draft_mode) {
      system(sprintf('touch progress_%s_%d', label, i))
    }
    if (exists('ks') && length(ks) > 0) {
      ks <- mapply(c, ks, new_draws, SIMPLIFY=FALSE)
    } else {
      ks <- new_draws
    }
  }
  return (ks)
}
ks.trig <- KsForMethod(InterpTrig, 'trig')
ks.delocalized <- KsForMethod(DelocalizedMatrix, 'deloc')

ListToDataFrame <- function(L, name) {
  d <- cbind(melt(do.call(cbind.data.frame, L)), type=name)
  names(d)[which(names(d) == 'variable')] <- 'num.draws'
  names(d)[which(names(d) == 'value')] <- 'x'
  return (d)
}
ks.all <- rbind(
    ListToDataFrame(ks.independent, 'Independent Draws'),
    ListToDataFrame(ks.trig, 'Trigonometric Interpolation'),
    ListToDataFrame(ks.delocalized, 'Delocalized (this work)'))
ks.figure.caption <- sprintf(
    paste('How faithfully does each Gaussian oscillator represent the normal',
          'distribution? This figure uses the Kolmogorov-Smirnov distribution',
          'to answer that question.  (Each curve is a histogram of %s draws.)',
          '(a) Independent draws from the normal distribution (which are not',
          'Gaussian oscillators) provide a basis for comparison.  As expected,',
          'taking more draws shifts the distribution towards smaller values,',
          'indicating a better fit.',
          '(b), (c), and (d) show the Gaussian oscillators for %d, %d, and %d',
          'draws, respectively, alongside the same number of independent',
          'draws.  Trigonometric interpolation is always more faithful than',
          'the same number of independent draws, but the delocalized',
          'oscillators this paper introduces are significantly better than',
          'both.'),
    formatC(num_oscillators, big.mark=',', format='fg'),
    equivalent_draws[1],
    equivalent_draws[2],
    equivalent_draws[3])
@

This motivates the following two-pronged approach: first, measure how well an individual animation represents the Gaussian; second, compute the \textit{distribution} of this metric across all animations in the family.
The Kolmogorov-Smirnov (KS) statistic --- essentially, the widest vertical gap between two cumulative distribution functions --- is a convenient, widely-used technique to compare two distributions.
The KS \textit{distribution} summarizes the fidelity of a Gaussian oscillator \textit{family}: distributions which tend toward smaller values indicate better techniques.\footnote{
  Of course, we could compute distributions for any other metric which compares probability distributions.
  The KS metric is not special; we chose it because it is both simple and widely used.
}

As a basis for comparison, consider the KS distribution for a given number $N$ of independent normal draws, without interpolation.\footnote{
  To be clear: these are not \textit{themselves} Gaussian oscillators; they are merely a benchmark.
}
Larger samples represent their distributions more faithfully; therefore, we expect their KS distributions to shift towards zero.
Figure \ref{fig:ks_distribution}(a) shows the results for \Sexpr{equivalent_draws[1]}, \Sexpr{equivalent_draws[2]}, and \Sexpr{equivalent_draws[3]} draws.
It is clear that larger samples indeed have smaller KS statistics.

The next three subfigures compare two types of Gaussian oscillator --- trigonometric interpolation, and the delocalized oscillators this paper introduces --- to the equivalent number of independent draws.
The figure shows that interpolating between the independent draws (orange curve) more faithfully represents the normal distribution than the independent draws themselves (green curve), since the orange curve puts more mass on lower KS values.
However, the delocalized oscillators (purple curve) are significantly better than either in every case shown here.

Thus, not only is their motion more smooth and natural, but the delocalized oscillators also represent the standard normal more faithfully.

However, this conclusion only holds for a fixed number of samples.
If more samples can be added continually (i.e., for an online animation), then the fidelity will continue to improve the longer the animation runs.
Interpolation has this capability, and delocalized oscillators do not.

<<ks_distribution, echo=FALSE, fig.cap=ks.figure.caption>>=
KsFigure <- function(i) {
  d <- ks.all[i, ]
  p <- (ggplot(data=d, aes(x=x, y=..density.., colour=type,
                           group=interaction(type, num.draws)))
        + geom_freqpoly(binwidth=diff(range(d$x))/60, size=line.size)
        + theme(legend.position='none')
        + scale_colour_brewer('', palette='Set2')
        + scale_x_continuous('Kolmogorov-Smirnov statistic', limits=c(0, 1))
        + scale_y_continuous('Density')
        )
}
independent.peaks <- data.frame(draws=equivalent_draws,
                                ks=c(0.5, 0.25, 0.125),
                                density=c(3.7, 5.5, 11.5))
colour.legend <- theme(legend.justification=c(1, 1),
                       legend.position=c(1, 1),
                       legend.margin=unit(0, 'mm'),
                       legend.title=element_blank())
suppressWarnings(grid.arrange(
    ncol=2,
    KsFigure(which(ks.all$type == 'Independent Draws'))
       + geom_text(inherit.aes=FALSE, hjust=0, data=independent.peaks,
                   aes(label=draws, x=ks, y=density))
       + ggtitle('(a) Independent draws'),
    KsFigure(which(ks.all$num.draws == 2))
       + colour.legend
       + ggtitle(sprintf('(b) %d draws', equivalent_draws[1])),
    KsFigure(which(ks.all$num.draws == 10))
       + colour.legend
       + ggtitle(sprintf('(c) %d draws', equivalent_draws[2])),
    KsFigure(which(ks.all$num.draws == 50))
       + colour.legend
       + ggtitle(sprintf('(d) %d draws', equivalent_draws[3]))))
@

\subsection{Kinematic Properties}
\label{sub:kinematic_properties}

Gaussian animations treat the curve as a quasi-physical moving object.
We therefore investigate the most basic kinematic properties of that motion: velocity and acceleration.
These distributions capture information which eludes the marginal \textit{position} distribution (which is the same by definition for all Gaussian oscillators).

<<kinematics_prep, include=FALSE>>=
approximate_derivative <- function(mat, t, deriv=0) {
  # Take the derivative with respect to the rows.
  if (deriv == 0) {
    return (mat)
  }
  mat.aug <- rbind(mat, mat[1, ])
  t.aug <- c(t, sum(tail(t, 2) * c(-1, 2)))
  approximate_derivative(diff(mat.aug) / diff(t.aug), t, deriv - 1)
}

graph <- function(label, interp.mat, deriv=0, scale=1, N.draws=3, seed=2,
                  show_keyframes=FALSE) {
  set.seed(seed)
  mat <- approximate_derivative(interp.mat, t=t.out, deriv=deriv)
  random.data <- matrix(rnorm(n=ncol(mat) * N.draws), ncol=N.draws)
  timetraces <- data.frame(y=as.vector(mat %*% random.data),
                           group=rep(1:N.draws, each=nrow(mat)),
                           t=t.out)
  first_timetrace <- timetraces[1:nrow(mat), ]
  p <- (ggplot(data=timetraces, aes(x=t, y=y, group=group))
        + geom_line(colour='#999999')
        + geom_line(data=first_timetrace)
        + scale_y_continuous("", breaks=scale * (-3):3)
        + scale_x_continuous("")
        + coord_cartesian(ylim=3 * ((pi / 2) ^ (2 * deriv)) * c(-1, 1))
        + theme(axis.text.x=element_blank())
        + theme(plot.margin=unit(rep(0.1, 4), "cm"))
        )
  if (show_keyframes) {
    p <- p + geom_point(data=data.frame(t=0:nrow(random.data),
                                        y=c(tail(random.data[, 1], 1),
                                            random.data[, 1])),
                        aes(x=t, y=y), inherit.aes=FALSE)
  }
  return (p)
}

graphs <- function(label, mat, n) {
  interpolating <- length(grep('Interpolation', label)) > 0
  list(graph(label, mat, N.draws=n, deriv=0, scale=2, show_keyframes=interpolating),
       graph(label, mat, N.draws=n, deriv=1, scale=5),
       graph(label, mat, N.draws=n, deriv=2, scale=10))
}

# Construct the graphs.
graphs.esg <- graphs(label="ESG Interpolation", mat=interp.esg, n=N.draws)
graphs.deloc <- graphs(label="Smooth Timetraces", mat=interp.deloc, n=N.draws)
quickie.text <- function(x, ...) textGrob(x, just='center', gp=gpar(fontsize=15), ...)
kinematics.caption <- sprintf(
  paste(
    'The basic kinematics of two types of Gaussian oscillators:',
    'the standard trigonometric oscillators, and the ``delocalized oscillators\'\' this paper introduces.',
    'Both populations are illustrated with %d sample paths (grey), one of which is randomly chosen and highlighted for illustration.',
    'The velocity and acceleration graphs for the Trigonometric oscillators clearly show that keyframes are treated specially.',
    'By contrast, all times for the delocalized oscillators are statistically equivalent.'),
  N.draws
  )
@

<<kinematics, echo=FALSE, fig.cap=kinematics.caption, fig.height=4>>=
grid.arrange(nullGrob(),
             quickie.text('Position'),
             quickie.text('Velocity'),
             quickie.text('Acceleration'),
             quickie.text('Trigonometric', rot=90), graphs.esg[[1]], graphs.esg[[2]], graphs.esg[[3]], 
             quickie.text('Delocalized\n(this paper)', rot=90), graphs.deloc[[1]], graphs.deloc[[2]], graphs.deloc[[3]],
             ncol=4,
             heights=c(0.2, 1, 1, 1),
             widths=c(0.23, 1, 1, 1))
@
Figure \ref{fig:kinematics} shows the results.
The velocity curves for interpolation change discontinuously at every keyframe.
This makes the acceleration effectively infinite there.
At every keyframe, the oscillator receives an ``impulse,'' which distractingly changes its motion.
By contrast, the delocalized oscillators' motion is perfectly fluid.
Each time is statistically equivalent to each other time --- not just for the oscillator's position, but for its velocity and acceleration too.

To understand these different behaviours, we view these oscillators as Gaussian processes, and compare their covariance functions $k(t_1, t_2)$.
Delocalized oscillators have a \textit{stationary} covariance (Equation \ref{eqn:do_covariance}): it depends only on $(t_2 - t_1)$, not on $t_1$ or $t_2$ individually.
Trigonometric interpolation has a non-stationary covariance:
\begin{equation}
  k_\text{TI}(t_1, t_2) = \sum_{i=1}^N b_i(t_1) b_i(t_2),
\end{equation}
where $b_i(t)$ is the $i$th basis function (see Figure \ref{fig:basis}).\footnote{
  Note that the covariance from one keyframe to the next (e.g., $k_\text{TI}(0, 1)$) is 0, as we would expect.
  But the covariance \textit{between} keyframes can be quite high, even for the same interval, e.g., $k_\text{TI}(0.5, 1.5) = \frac{1}{2}$.
}
This non-stationarity gives $k_\text{TI}(t_1, t_2)$ a stepped, ``blocky'' appearance (see Figure \ref{fig:cov_mat}(a)).

<<cov_mat_prep, include=FALSE>>=
max_x <- 4
N.points <- 200
x <- seq(from=0, to=max_x, length.out=N.points)

CovarianceMatrixPlot <- function(title, FUN) {
  (ggplot(data=melt(outer(x, x, function(a, b) FUN(a, b))),
          aes(x=x[Var2], y=x[Var1], fill=value))
   + geom_raster()
   + ggtitle(title)
   + scale_fill_gradient2('Covariance', limits=c(-1, 1))
   + coord_fixed()
   + theme(axis.title.y=element_text(angle=0))
   + scale_x_continuous(expand=c(0, 0), expression(t[1]))
   + scale_y_reverse(expand=c(0, 0), expression(t[2]))
   + geom_abline(slope=-1, intercept=1)
   + geom_abline(slope=-1, intercept=-1)
  )
}

interpolation_plot <- CovarianceMatrixPlot(
    'a) Trigonometric\nInterpolation',
    function(a, b) {
      COS_2 <- function(a, b) cos(0.5 * pi * (a - b))
      total <- 0
      for (i in 0:max_x) {
        total <- total + weight(i, a, COS_2) * weight(i, b, COS_2)
      }
      return (total)
    })
delocalized_plot_loop <- CovarianceMatrixPlot(
    'b) Delocalized oscillators\n(Short loop)',
    function(a, b, n=4) {
      total <- 0
      for (i in 1:(n/2)) {
        total <- total + cos(2 * pi * i * (a - b) / n)
      }
      2 * total / n
    })
delocalized_plot_inf <- CovarianceMatrixPlot(
    'c) Delocalized oscillators\n(Non-looping)',
    function(a, b) {
      z <- pi * (a - b)
      return (ifelse(a == b, 1, sin(z) / z))
    })
cov_mat.caption <- paste(
    'Example covariance functions $k(t_1, t_2)$ for various types of animations.',
    'The black bars represent the unit-time autocorrelation; the ideal covariance function would be zero everywhere outside this central stripe.',
    'a) Trigonometric interpolation. The covariance vanishes at long times, but is nonstationary (leading to jerky motion).',
    'b) Looping delocalized oscillators with a short period.  The covariance is stationary, but intermediate times have strong anticorrelation.',
    'c) Delocalized oscillators in the infinite-period limit (i.e., non-looping).  The anticorrelation is still nonzero, but much smaller.'
                         )
@

<<cov_mat, echo=FALSE, fig.cap=cov_mat.caption, fig.height=4>>=
no.legend <- theme(legend.position='none')
grid.arrange(nrow=1, widths=c(1, 1, 1, 0.35),
             interpolation_plot + no.legend,
             delocalized_plot_loop + no.legend,
             delocalized_plot_inf + no.legend,
             LegendOnly(interpolation_plot))
@

To avoid any frames being treated specially, we should prefer Gaussian oscillators with stationary covariance.
If we further want motion that is smooth and natural, we should choose a covariance which is continuous and differentiable at $t_1 = t_2$.\footnote{
  Since covariance functions are symmetric, this implies the slope should be 0 here.
}

\subsection{Time Correlation}
\label{sub:time_correlation}

In Monte Carlo simulations, autocorrelation is undesirable.
In Gaussian animations, it is desirable at short time intervals; otherwise, the motion could not be continuous.
Nevertheless, minimizing autocorrelation at longer times is still a virtue.

A useful metric is the autocorrelation at time intervals of one unit.
For interpolation, this is the mean time between independent draws; hence, we could reasonably hope for a unit-time autocorrelation near 0.
The best possible result would be an autocorrelation of \textit{exactly} 0, for all time intervals longer than this.\footnote{
  If the animation is periodic, this condition applies to the \textit{smallest} time interval between two points.
  For example, an interval of one period should be treated as an interval of 0.
}

Trigonometric interpolation achieves perfectly uncorrelated unit-time intervals, but only for keyframes.
Between the keyframes, the same draw $\varepsilon_i$ contributes to both values, meaning the correlation will be nonzero.
In the worst case, the two values at $\Delta t = \pm \frac{1}{2}$ relative to a keyframe have a correlation of $\frac{1}{2}$, which is rather high.

The unit-time autocorrelation for delocalized oscillators is
\begin{equation}
  \langle \xi(t) \xi(t + 1) \rangle = \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
  \left( \frac{2 \pi i}{N} \right) = -\frac{2}{N}.
\end{equation}
Thus, delocalized oscillators are slightly anticorrelated at single-unit intervals\footnote{
This is valid at all times, since unlike interpolated animation, the delocalized oscillators are stationary.}.
For small $N$, this autocorrelation is worse than interpolation, even becoming as high as -1 (perfect anticorrelation) for the $N = 2$ case.\footnote{
  This anticorrelation of -1 clearly also applies to great circle animation.
  One time unit corresponds to half the period; therefore, all values are negated.
}
However, for $N > 4$, delocalized oscillators are always better than the worst case for interpolation.
Higher $N$ values improve the autocorrelation (and, as noted earlier, the fidelity as measured by the KS distribution).

These benefits of higher $N$-values motivate us to consider the infinite-$N$ limit.
The sum in the covariance function (Equation \ref{eqn:do_covariance}) can be converted to an integral by taking $x \equiv \frac{2i}{N}$:
\begin{align}
  \lim_{N \rightarrow \infty} \frac{2}{N} \sum_{i=1}^{N/2} \cos \left( \frac{2 \pi i (t_2 - t_1)}{N} \right)
    &= \int\limits_0^1 \cos \left( \pi (t_2 - t_1) x \right) \, dx \\
    &= \sinc \left( \pi \left( t_2 - t_1 \right) \right),
\end{align}
where $\sinc(0) = 1$, and $\sinc(x) \equiv \sin(x) / x \,\,\,\, \forall x \ne 0$.

Thus, in the infinite limit, the autocorrelation vanishes at all intervals which are nonzero integer multiples of 1.
The disadvantage is the considerable computational expense, both in time and space (as mentioned in Section \ref{sub:connection_with_gaussian_processes}).
Furthermore, most intervals longer than one unit have a small but nonzero autocorrelation (see Figure \ref{fig:cov_mat}(c)).

\subsection{Summary of Metrics}
\label{sub:summary_of_metrics}

We now summarize the relative merits of the various Gaussian animation techniques.

The most important metric is the Kolmogorov-Smirnov distribution, because it measures how well individual animations represent the standard normal.
Delocalized oscillators and great circles exhibit significantly better KS distributions than interpolation, if the number of independent samples is fixed.
Taking more samples improves every technique except great circles, which cannot use more than two.
These considerations point to delocalized oscillators as the most faithful offline technique considered here.
Interpolation, however, performs better in the online case, since it can perpetually incorporate new random samples.

The next most important criterion is the quality of motion.
Ideally, no frames should have preferred status, and the motion should be smooth.
Delocalized oscillators and great circles excel here, while interpolation produces distracting, jerky motion.
(Note that any future techniques with smooth, stationary covariance would also yield excellent quality motion.)

Finally, we want to maximize the mutual independence of all frames, by minimizing autocorrelation beyond what is necessary.
Here, no single technique is clearly best.
Interpolation limits autocorrelation to intervals of at most two units, but the precise distance is different for different frames.
Delocalized oscillators with long period have low autocorrelation at intervals beyond 1 unit, but it never quite reaches zero.

Overall, delocalized oscillators considerably improve on the state of the art for offline Gaussian animations.
They represent the normal distribution much more faithfully, and they do so with fluid, natural motion, treating no frames specially.

\section{Future Work}
\label{sec:future_work}

\subsection{Animating non-Gaussian distributions}
\label{sub:animating_non_gaussian_distributions}

This paper has shown how to visualize arbitrary Gaussian distributions using continuous animations.
The question naturally arises which other distributions can be visualized in this way.
Some distributions are clearly impossible, but several others seem tractable.

First, consider a ``nearly-Gaussian'' distribution, i.e., one whose probability density $\rho(x)$ differs from the Gaussian density $\nu(x)$ by some known factor (which naturally depends on $x$).
The time which a standard Gaussian oscillator spends at $x$ is proportional to $\nu(x)$.
To make it proportional to $\rho(x)$ instead, we can modulate the delay before the next frame by a factor $(\rho(x) / \nu(x))$.
In essence, we are warping time to capture the subtle departures of the true distribution from the Gaussian.

A more challenging case is distributions with multiple regions that are connected only thinly (or even disconnected entirely).
Time warping seems infeasible here for several reasons.
First, the oscillator would need to move infinitely quickly in the intermodal region.
Furthermore, any single Gaussian is likely to be a very poor approximation to the distribution as a whole, so the time warping would need to be extreme.

A better alternative would be to show a separate animation for each region.
A Gaussian can better approximate individual modes rather than the entire distribution.
And the total probability mass of each mode can be indicated beside its animation.

Discrete distributions present a greater challenge: obviously, they cannot be represented by continuous animations.
However, even here there are special cases which admit partial success.
Consider a Poisson distribution (representing, e.g., neutron counts in a scattering curve \citep{hklm2010}).
If we perform an Anscombe transform \citep{anscombe1948}, the transformed data follows the Gaussian distribution to an excellent approximation.
One could animate the transformed distribution with Gaussian oscillators, ``un-transform'' the result, and round to the nearest integer.
While the result would not be truly continuous, it would change in an ordered way which is easy for the eye to follow.

Animations are highly useful for visualizing complex distributions.
With the Gaussian case now well in hand, other distributions beg to be explored.

\subsection{Alternative time-domain covariance functions}
\label{sub:alternative_time_domain_covariance_functions}

Let us consider again the criteria outlined in Section \ref{sec:comparison}, and try to engineer a superior Gaussian oscillator.

The most important criterion is the KS distribution, since it is vital that the animation represents the normal distribution faithfully.
The best possible way to fulfill this is to use an online technique, which can perpetually incorporate new random samples.
We also want smooth, natural motion, with no frames treated specially; this implies a smooth, stationary covariance function.
Finally, the autocorrelation should be zero for all intervals longer than one unit.

We can satisfy all these criteria simultaneously if our covariance has \textit{compact support}.
For example, consider this piecewise polynomial covariance function:
\begin{equation}
  k(\tau) =
  \begin{cases}
    (1 - \tau)^6(12.8 \tau^3 + 13.8 \tau^2 + 6 \tau + 1) & 0 \le \tau \le 1 \\
    0 & \tau > 1,
  \end{cases}
\end{equation}
where $\tau$ is the time interval magnitude $|t_2 - t_1|$.
Curves sampled from this distribution are twice differentiable everywhere \citep{3569}; hence, their motion will be smooth and natural.
Their autocorrelation completely vanishes at all intervals longer than one unit: the best possible result.
The covariance is stationary, so no frames can be special in any way.
Finally, they permit infinite non-repeating animations with finite resources\footnote{
  The appendix explains in detail how to do this.
}, just as interpolation does.

This, then, is the bright future we envision for animated uncertainty visualization.
\texttt{R} packages which output interactive web apps, such as \texttt{shiny} \citep{shiny}, can be augmented to support animated output.
Rather than a static image, or a finite looping \texttt{.gif}, the curve continuously moves in a smooth and natural way: never repeating its position, and visualizing more and more of the distribution.

\section{Conclusions}
\label{sec:conclusions}

Animations are a useful and beautiful technique to visualize uncertainty in curves and surfaces.
This paper introduced a unifying concept, the ``Gaussian oscillator,'' to compare existing techniques and pave the way for future ones.
For the first time, we have measured the quality of individual animations rather than populations; in so doing, we have uncovered significant differences between techniques.
We also introduced a new technique, \textit{delocalized oscillators}, with many advantages: its motion is fluid and natural; it is efficient to compute; and it represents the underlying distribution more completely.
Finally, we have revealed that Gaussian animations were simply a special case of Gaussian processes all along.
We expect this relationship will bear much fruit and lead to even better animations in the future.

\section{Acknowledgements}
\label{sec:acknowledgements}

I gratefully acknowledge the help and support which made this work possible.
First, I am thankful to the National Institute of Standards and Technology, where I was employed when I initially developed delocalized oscillators.
Bob McMichael greatly expanded the utility of this work by pointing out that it applies to parametric models, not just Gaussian processes.
Finally, Yarin Gal introduced me to the delightful and mesmerizing animations of Philipp Hennig.

This paper is written using the \texttt{knitr} package \citep{knitr}, which enabled me to include the \texttt{R} source code to generate my figures directly in the source text.
This source is available on github: \url{https://github.com/chiphogg/paper_gaussian_oscillators}.
I am also thankful to Melissa DeLucchi and Yang Feng, who provided valuable feedback on a draft of this paper.

\section*{Appendix: efficient online animations}
\label{sec:appendix_efficient_online_animations}

Suppose we have a Gaussian oscillator timetrace, consisting of $n$ points.
Can we extend the timetrace by computing the $(n + 1)$st?
If so, what is the computational cost?

This appendix will show that we \textit{can} extend Gaussian oscillators, but the computational cost is unacceptably high.
Most Gaussian oscillators are $\order(n^2)$ in both time and space.
Oscillators with \textit{compact support} are the exception: they can be computed very efficiently ($\order(1)$ with respect to $n$).
This suggests that compact support oscillators represent the future of online Gaussian animations.

\subsection*{The general case}
\label{sub:the_general_case}

Let us treat the timetrace as a Gaussian process.
This means we have generated $n$ \iid~standard normal variates, $\xi^{(n)} \equiv \left\{\xi_1, \cdots, \xi_n\right\}$, and the lower-Cholesky decomposition $L^{(n)}$ of the covariance matrix $K^{(n)}$.
The first $n$ points are given by
\begin{equation}
  \alpha^{(n)} = L^{(n)} \xi^{(n)}.
\end{equation}

The covariance matrix $K^{(n + 1)}$ for the first $(n + 1)$ points can be written in block diagonal form:
\begin{equation}
  K^{(n + 1)} = \left[
    \begin{array}{cc}
      K^{(n)} & k^{(n)} \\
      k^{(n)\transpose} & 1
    \end{array}
    \right],
\end{equation}
where $K^{(n)}$ is the covariance matrix for the first $n$ points, and $k^{(n)}$ is a vector giving the covariance of the new point with each old point.
The Cholesky root can be similarly decomposed:
\begin{equation}
  L^{(n + 1)} = \left[
    \begin{array}{cc}
      L^{(n)} & 0 \\
      \ell^{(n)\transpose} & \gamma
    \end{array}
    \right],
\end{equation}
where $L^{(n)}$ is the Cholesky root of $K^{(n)}$, $\ell^{(n)}$ is an unknown vector, and $\gamma$ is an unknown scalar.

The $(n + 1)$st point is
\begin{equation}
  \alpha_{n + 1} = \left[
    \begin{array}{cc}
      \ell^{(n)\transpose} & \gamma
    \end{array}
  \right] \xi^{(n + 1)}.
  \label{eqn:next_point}
\end{equation}
At this point, we drop the $(n)$ and $(n + 1)$ superscripts for convenience.
The definition of the Cholesky root gives the following block matrix equation:
\begin{equation}
  \left[
    \begin{array}{cc}
      LL^\transpose & L\ell \\
      \ell^\transpose L^\transpose & \ell^\transpose \ell + \gamma^2
    \end{array}
  \right] = \left[
    \begin{array}{cc}
      K & k \\
      k^\transpose & 1
    \end{array}
  \right],
\end{equation}
which yields the vector equation
\begin{equation}
  L \ell = k
  \label{eqn:go_cov_old_new}
\end{equation}
and the scalar equation
\begin{equation}
  \ell^\transpose \ell + \gamma^2 = 1
\end{equation}
Since $L$ is lower-triangular and $k$ is known, Equation \ref{eqn:go_cov_old_new} can be trivially solved for $\ell$, and we can compute the $(n + 1)$st point.

However, the computational cost is catastrophic.
Imagine we leave the animation running, so that $n$ continues to increase.
The time to compute each new point by Equation \ref{eqn:go_cov_old_new} is $\order(n^2)$.
The storage also increases as $\order(n^2)$, since we require the entire matrix $L^{(n)}$.
This animation will consume all system resources at an ever-increasing pace, while delivering new points ever more slowly.

\subsection*{Compact Support}
\label{sub:compact_support}

To prevent this disaster, suppose the covariance matrix has compact support, so that the new value is correlated only with a finite number of previous values; say, at most $m$.
This means that $\left\{k_1, \cdots, k_{n - m} \right\}$ are all 0.
Equation \ref{eqn:go_cov_old_new} implies that $\left\{\ell_1, \cdots, \ell_{n - m} \right\}$ are also 0.

This yields enormous computational savings.
Since $L$ is lower-triangular, only the final $m \times m$ submatrix actually contributes to the calculation of $\ell$ (by Equation \ref{eqn:go_cov_old_new}).
Similarly, Equation \ref{eqn:next_point} only uses the $m$ most recent random variates, $\left\{\xi_{n - m + 1}, \cdots, \xi_n\right\}$; all the preceding ones can be discarded.
The important point to notice is that our computational requirements no longer grow without limit as the animation continues to run.

We can do still better.

Imagine now that the time interval between successive points is constant.
Then each row of $K$ (after the $m$th) is equivalent to the previous row, shifted by one column.
The same will be true for $L$.
Since each row has the same $m$ nonzero values, they need only be computed once.

We also only need to store $m$ random values, since the $n$th value can replace the $(n - m)$th (which is no longer needed).
The following R snippet\footnote{Note that this code is intended only to illustrate the concept; it is too inefficient for a real implementation.} illustrates how to generate an infinite timetrace, given a precomputed vector \texttt{ell} of length \texttt{m}:
<<compact_support_timetrace, echo=TRUE, eval=FALSE>>=
xi <- rnorm(m)
while (TRUE) {
  alpha <- xi %*% ell
  UpdateAnimation(alpha)
  xi <- c(tail(xi, -1), rnorm(1))
}
@

To summarize: extending an $n$-point Gaussian oscillator timetrace is generally $\order(n^2)$ in both time and space, which is unacceptable.
But if the Gaussian oscillator has compact support, both costs drop to $\order(m^2)$ (where $m$ is small and constant).
And if we furthermore evaluate that oscillator only at constant time intervals, all costs drop further to $\order(m)$.

\bibliographystyle{apalike}
\bibliography{biblio}
\end{document}
