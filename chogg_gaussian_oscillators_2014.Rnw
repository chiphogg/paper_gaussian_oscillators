\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amssymb}
\usepackage{natbib}

\DeclareMathOperator{\sinc}{sinc}
\newcommand{\transpose}{\intercal}

\begin{document}

\title{Visualizing uncertain curves and surfaces via Gaussian Oscillators}
\author{Charles~R.~Hogg~III}
\date{August~2014}
\maketitle

<<global_setup, cache=FALSE, include=FALSE>>=
require(ggplot2)
require(knitr)
require(gridExtra)

source('interpolation.R')

# Smart caching.
opts_chunk$set(cache=TRUE, autodep=TRUE)

# ggplot2 options.
theme_set(theme_grey(base_size=12))
line.size <- 1

# For reproducibility.
set.seed(1)
@

\section{Introduction}
\label{sec:introduction}

Curves and surfaces abound in the sciences.
It is vital to quantify their uncertainty -- usually, by computing a probability distribution.
It is equally vital to visualize that uncertainty.
Here, curves and surfaces present considerable challenges.

The most common technique is to plot upper and lower credible intervals along with the estimated curve.
This shows considerable information at a glance.
However, as a pointwise technique, it discards correlations between points: for example, it cannot distinguish between slowly and rapidly varying curves.
Also, this technique is difficult to apply to surfaces, since they can easily obscure one another.

Showing individual draws from the distribution addresses the former problem nicely.
However, there is a fundamental tension: showing too few curves will not fully represent the distribution, while showing too many quickly clutters the figure.
What's more, the problem of obscured surfaces is only made more acute.

The solution is to show the draws one at a time.

Generally, consecutive draws will be very different in highly uncertain regions, leading to distracting, ``jumpy'' animations.
However, consecutive draws can be arranged to be very similar (by introducing suitably decaying time correlation), resulting in \textit{continuous} animations.

This paper focuses on curves and surfaces whose underlying distribution is Gaussian.
This is not nearly as restrictive as it might seem.
It includes Gaussian Processes\cite{3569}, a flexible nonparametric class of models.
Furthermore, parametric models often have a single posterior mode, which is Gaussian to leading order.
Continuous paths through the Gaussian parameter space correspond to continuous animations of the function those parameters govern.

The present author is aware of two main approaches in the literature.

Historically, most animations have been based on \textit{interpolation}\cite{skilling1992,esg1997}.
A series of independent draws from the Gaussian are generated and displayed at regular time intervals; these are called the \textit{keyframes}.
The frames at intervening times are weighted averages of the nearest two keyframes.
The weighting favours the first keyframe at the beginning of the interval, and the second keyframe near the end, so that the curve or surface changes continuously from one keyframe to the next.
Figure \ref{fig:interpolation}(a) illustrates the idea for a simple 1-dimensional Gaussian using three possible interpolating families: \textit{linear}, \textit{cubic spline}, and \textit{trigonometric}.

<<interpolation_plots, include=FALSE>>=
quantiles <- c(0.1, 0.5, 0.9)
num.interpolated.curves <- 5e2  # Make it bigger later
num.points <- 500
num.keyframes <- 4
interpolation.caption <- sprintf(
    paste("Interpolation illustrated for a standard 1-D Gaussian.",
          "(a) A single set of normal draws interpolated by three methods.",
          "(b) Pointwise quantiles for interpolating %s random draws."),
    formatC(num.interpolated.curves, big.mark=',', format='fg'))

interp.matrix.functions <- list(Linear=InterpLinear,
                                Spline=InterpSpline,
                                Trigonometric=InterpTrig)

p.concept <- InterpolationConceptPlot(
    interp.matrix.functions=interp.matrix.functions,
    n.points=num.points,
    n.keyframes=num.keyframes,
    size=line.size)
p.interp <- InterpolationQuantilePlot(
    interp.matrix.functions=interp.matrix.functions,
    quantiles=quantiles,
    n.draws=num.interpolated.curves,
    n.points=num.points,
    n.keyframes=num.keyframes,
    size=line.size)
@

<<interpolation, cache=FALSE, echo=FALSE, fig.cap=interpolation.caption>>=
grid.arrange(p.concept + scale_colour_brewer('', palette='Set2'),
             p.interp + scale_colour_brewer('', palette='Set2'),
             ncol=1)
@

Unfortunately, interpolation can easily yield incorrect statistics between the keyframes.
In particular, the variance is usually underestimated.
Figure \ref{fig:interpolation}(b) shows that only the trigonometric approach\cite{esg1997,skilling1992} correctly preserves statistical properties between keyframes.

An important advance was made by Hennig\cite{hennig-tr-is-8}, who generated circular paths with marginal normal distributions.
A draw from the $m$-dimensional Gaussian formed one point on the circle.
A second independent draw set the direction of traversal.

Since no point on a circle is special, every frame in these circular timetraces is fully equivalent to every other.
To the best of the author's knowledge, Hennig's paper is the first published example of a smooth, keyframe-free Gaussian animation.

Each approach has advantages and weaknesses.
Interpolation is flexible: it can incorporate arbitrarily many independent draws, yielding paths which better cover and represent the Normal distribution.
The downside is that not all frames are equivalent: keyframes have preferred status.
The motion changes discontinuously at the keyframes, and intermediate frames can be excessively correlated.

Great Circle animations\cite{hennig-tr-is-8} do not have this defect; they treat all frames equally in every way.
Though based on just two independent draws, they represent the normal distribution better than ($\cdots$).

This paper will compare these methods quantitatively for the first time.
It will then introduce a new method for Gaussian animations, which combines the best advantages of the existing approaches.
The new method is as flexible and extensible as Trigonometric Interpolation, yet as smooth and natural as the Great Circles.

\section{Gaussian Oscillators}
\label{sec:gaussian_oscillators}

This section will define and explain the \textit{Gaussian Oscillator}: the key concept for analyzing Gaussian animations.

First, consider a $m$-dimensional Gaussian, with mean vector $\mu$ and covariance matrix $K$.
To take a single draw from this distribution, we require the lower-triangular Cholesky decomposition $L$ of $K$, such that $LL^\intercal = K$.
If $\xi = \left( \xi_1, \cdots, \xi_m \right)^\intercal$ represents $m$ i.i.d. draws from the \textit{standard} normal $N(0, 1)$, then $\alpha \equiv \mu + L\xi$ is a draw from $N(\mu, K)$.

Now imagine that each $\xi_i$ is a random function of time, $\xi_i(t)$, such that $\xi_i(t) \sim N(0, 1) \,\, \forall t$.
It follows that $\alpha(t) \sim N(\mu, K) \,\, \forall t$.
Thus, the problem of animating arbitrary multidimensional Gaussian distributions reduces to that of animating a single-dimensional standard Gaussian.
This latter problem is the focus of the rest of the paper.

This motivates us to define a \textit{Gaussian Oscillator} as a stochastic process, $\xi(t)$, with the following properties.
\begin{equation}
  \xi(t) \sim N(0, 1) \,\, \forall t
  \label{eqn:go_marginal}
\end{equation}
\begin{equation}
  \lim_{\Delta t \rightarrow 0} \langle \xi(t) \xi(t + \Delta t) \rangle = 1 \,\, \forall t
  \label{eqn:go_continuous}
\end{equation}
Equation \ref{eqn:go_marginal} means that every frame of the animations is marginally Gaussian.
Equation \ref{eqn:go_continuous} means that the animation is \textit{continuous}.

Most $m$-dimensional Gaussian animations can be analyzed by combining $m$ independent Gaussian oscillators.
Great Circle animations are an exception, since they sample directly from the $m$-dimensional distribution.
Each individual dimension of a Great Circle animation fulfills the definition of a Gaussian Oscillator.
However, the oscillators are not independent (for example, the $m$-dimensional vector is constrained to have constant magnitude).

\subsection{Connection with Gaussian Processes}
\label{sub:connection_with_gaussian_processes}

Readers familiar with the concept will have noticed that Gaussian Oscillators are also Gaussian \textit{Processes}\cite{3569}, with a few constraints.
Specifically, the mean function $\mu(t)$ is zero everywhere, and the covariance function $k(t, t')$ is 1 when $t = t'$ (and continuous in that neighbourhood).

Remarkably, nobody seems to have pointed out this connection before.
It is extremely useful, since future Gaussian animation research can leverage the well-studied menagerie of covariance functions\cite{3569}.

There is some danger of confusion here, because often the curves and surfaces being animated are also modeled using Gaussian Processes.
The difference is that the Gaussian Oscillator is a \textit{time-}domain Gaussian Process, while the curve or surface lives in the \textit{space} domain\footnote{
The ``space'' here might be an abstract space.  It might even be (physical) time --- for example, with time-series analysis.  However, the point is that this is distinct from the ``animation time'' domain where the Gaussian Oscillator lives.
}.

Treating Gaussian Oscillators as Gaussian Processes is likely to be more useful for theoretical analysis than practical computations.
Consider a trigonometric interpolation of $k$ keyframes and $n$ total frames; typically, $n \gg k$.
Computing $\xi(t)$ as a Gaussian Process requires $n$ i.i.d. normal draws, and an $\mathcal{O}(n^3)$ matrix inversion!
By contrast, simple interpolation requires only $k$ normal draws (and simple sines and cosines instead of the matrix inversion).

\section{A New Way to Generate Animations}
\label{sec:a_new_way_to_generate_animations}

The new animations in this paper are most naturally compared to the interpolation technique.
Both techniques involve generating some number $N$ of i.i.d. standard normal variates, then deterministically producing a continuous timetrace.
With interpolation, each variate influences only the small region between its nearest neighbours.
In this paper's technique, each variate's invluence is spread out across the entire timetrace.

Explicitly, given $N$ i.i.d. normal variates $\{\epsilon_1, \cdots, \epsilon_n\}$, the corresponding \textit{delocalized oscillator} $f(t)$ is given by
\begin{equation}
  \label{eqn:delocalized}
  f(t) = \sqrt{\frac{2}{N}} \sum\limits_{i=1}^{N/2} \left[
  \epsilon_{2i-1} \sin \left( \frac{2 \pi i t}{N} \right) +
  \epsilon_{2i} \cos \left( \frac{2 \pi i t}{N} \right)
\right].
\end{equation}
Note that this is only defined for even $N$, since every $\sin(\cdot)$ needs a corresponding $\cos(\cdot)$.
Considering the application, this restriction seems mild.

Equation \ref{eqn:delocalized} superficially resembles a Fourier series, in that it is a weighted sum of sines and cosines.

((Elaborate on differences))

(((more terms -> longer period, NOT finer motion)))

(((orthogonality: red herring! not really needed for different periods)))

(((coefficients also decrease with increasing $N$)))

To verify that $f(t)$ is a Gaussian Oscillator, the relations $\langle \epsilon_i \rangle = 0$ and $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$ will be useful.
First, note that $f(t)$ is a sum of Gaussian random variables at each time $t$; therefore, $f(t)$ also has a Gaussian distribution.
This means that its mean $\langle f(t) \rangle$ and covariance $\langle f(t_1) f(t_2) \rangle$ characterize it completely.

From the definition in Equation \ref{eqn:delocalized}, and the property $\langle \epsilon_i \rangle = 0$, it is clear that $\langle f(t) \rangle = 0$.
The covariance requires more work.
It involves a double sum, but since $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$, only the $i=j$ terms survive:
\begin{align}
  \langle f(t_1) f(t_2) \rangle &= \frac{2}{N} \sum\limits_{i=1}^{N/2}
    \left[
      \sin \left( \frac{2 \pi i t_1}{N} \right) \sin \left( \frac{2 \pi i t_2}{N} \right) +
      \cos \left( \frac{2 \pi i t_1}{N} \right) \cos \left( \frac{2 \pi i t_2}{N} \right)
    \right] \\
  &= \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
    \left( \frac{2 \pi i (t_2 - t_1)}{N} \right).
    \label{eqn:do_covariance}
\end{align}
Note that this depends only on $(t_2 - t_1)$: these Gaussian Oscillators are \textit{stationary}, so no keyframes are singled out.
Note too that $\langle f(t)^2 \rangle = 1$, as required for Gaussian Oscillators.

\section{Comparison}
\label{sec:comparison}

\subsection{Basis functions}
\label{sub:basis_functions}

<<common_prep, include=FALSE>>=
# Parameters to govern the output for the basis and kinematics plots.
N.out <- 400
N.frames <- 4
N.draws <- 20
t.out <- seq(from=0, to=N.frames, length=N.out)

weight <- function(t.in, t.out, FUN) {
  ifelse(abs(t.in - t.out) > 1, 0, FUN(t.in, t.out))
}
periodic_weight <- function(t.in, t.out, period, FUN) {
  # Hack to make the basis functions periodic (instead of always 0 at t=0).
  pmax(weight(t.in, t.out, FUN), weight(t.in, t.out - period, FUN))
}

DelocalizedMatrix <- function(N.frames, t) {
  N <- N.frames / 2
  cbind(
    outer(t, 1:N, function(t, i) sin(pi * i * t / N)),
    outer(t, N:1, function(t, i) cos(pi * i * t / N))) / sqrt(N)
}
InterpolationMatrixESG <- function(N.frames, t.out) {
  t.frames <- 1:N.frames
  outer(t.out, t.frames, 
    function(x, y) {
      periodic_weight(x, y, N.frames, function(a, b) cos((a - b) * pi / 2))
    })
}

# Matrices.
interp.esg <- InterpolationMatrixESG(N.frames, t.out)
interp.deloc <- DelocalizedMatrix(N.frames, t.out)
@

Both interpolation and delocalized oscillators can be viewed as a linear combination of basis functions with random coefficients.
Comparing the basis functions for each method reveals the key differences between them.
Figure (put a figure here) shows these basis functions.

<<basis_prep, include=FALSE>>=
basis.colors <- scale_colour_brewer(palette='Set2')
basis.theme <- theme(legend.position='none')
basis.plot <- function(mat, title) {
  d <- data.frame(y=as.vector(mat), t=t.out,
    label=rep(paste0('a', 1:N.frames), each=N.out))
  p <- (ggplot(data=d, aes(x=t, y=y, colour=label))
    + geom_hline(yintercept=0)
    + geom_line(size=1)
    + scale_x_continuous('', limits=c(0, N.frames))
    + scale_y_continuous('', limits=c(-1, 1), breaks=(-1):1)
    + basis.colors
    + basis.theme
    + ggtitle(title)
    + theme(axis.text.x=element_blank())
    )
  return (p)
}
basis.caption <- sprintf(
  paste(
    'Basis functions for two types of Gaussian Oscillators (with %d independent draws).'),
  N.frames
  )
@

<<basis, echo=FALSE, fig.cap=basis.caption, fig.height=4>>=
grid.arrange(ncol=1,
  basis.plot(interp.esg, 'a) Trigonometric basis functions'),
  basis.plot(interp.deloc, 'b) Delocalized basis functions'))
@

(Discuss them.)

\subsection{Kinematic Properties}
\label{sub:kinematic_properties}

One way to compare Gaussian Oscillators is to compute their most basic kinematic properties: velocity and acceleration.
These distributions capture information which eludes the marginal \textit{position} distribution (which is the same by definition for all Gaussian Oscillators).

<<kinematics_prep, include=FALSE>>=
approximate_derivative <- function(mat, t, deriv=0) {
  # Take the derivative with respect to the rows.
  if (deriv == 0) {
    return (mat)
  }
  mat.aug <- rbind(mat, mat[1, ])
  t.aug <- c(t, t[1])
  approximate_derivative(diff(mat.aug) / diff(t.aug), t, deriv - 1)
}

graph <- function(label, interp.mat, deriv=0, scale=1, N.draws=3, seed=2,
                  show_keyframes=FALSE) {
  set.seed(seed)
  mat <- approximate_derivative(interp.mat, t=t.out, deriv=deriv)
  random.data <- matrix(rnorm(n=ncol(mat) * N.draws), ncol=N.draws)
  timetraces <- data.frame(y=as.vector(mat %*% random.data),
                           group=rep(1:N.draws, each=nrow(mat)),
                           t=t.out)
  first_timetrace <- timetraces[1:nrow(mat), ]
  p <- (ggplot(data=timetraces, aes(x=t, y=y, group=group))
        + geom_line(colour='#999999')
        + geom_line(data=first_timetrace)
        + scale_y_continuous("", breaks=scale * (-3):3)
        + scale_x_continuous("")
        + coord_cartesian(ylim=3 * ((pi / 2) ^ (2 * deriv)) * c(-1, 1))
        + theme(axis.text.x=element_blank())
        + theme(plot.margin=unit(rep(0.1, 4), "cm"))
        )
  if (show_keyframes) {
    p <- p + geom_point(data=data.frame(t=0:nrow(random.data),
                                        y=c(tail(random.data[, 1], 1),
                                            random.data[, 1])),
                        aes(x=t, y=y), inherit.aes=FALSE)
  }
  return (p)
}

graphs <- function(label, mat, n) {
  interpolating <- length(grep('Interpolation', label)) > 0
  list(graph(label, mat, N.draws=n, deriv=0, scale=2, show_keyframes=interpolating),
       graph(label, mat, N.draws=n, deriv=1, scale=5),
       graph(label, mat, N.draws=n, deriv=2, scale=10))
}

# Construct the graphs.
graphs.esg <- graphs(label="ESG Interpolation", mat=interp.esg, n=N.draws)
graphs.deloc <- graphs(label="Smooth Timetraces", mat=interp.deloc, n=N.draws)
quickie.text <- function(x, ...) textGrob(x, just='center', gp=gpar(fontsize=15), ...)
kinematics.caption <- sprintf(
  paste(
    'The basic kinematics of two types of Gaussian Oscillators:',
    'the standard trigonometric oscillators, and the ``delocalized oscillators\'\' this paper introduces.',
    'Both populations are illustrated with %d draws (grey), one of which is randomly chosen and highlighted for illustration.',
    'The velocity and acceleration graphs for the Trigonometric oscillators clearly show that keyframes are treated specially.',
    'By contrast, all times for the Delocalized oscillators are statistically equivalent.'),
  N.draws
  )
@

<<kinematics, cache=FALSE, echo=FALSE, fig.cap=kinematics.caption, fig.height=4>>=
grid.arrange(nullGrob(),
             quickie.text('Position'),
             quickie.text('Velocity'),
             quickie.text('Acceleration'),
             quickie.text('Trigonometric', rot=90), graphs.esg[[1]], graphs.esg[[2]], graphs.esg[[3]], 
             quickie.text('Delocalized\n(this paper)', rot=90), graphs.deloc[[1]], graphs.deloc[[2]], graphs.deloc[[3]],
             ncol=4,
             heights=c(0.2, 1, 1, 1),
             widths=c(0.23, 1, 1, 1))
@
Figure ((make-a-figure!!)) shows the results.
The velocity distribution for interpolation changes discontinuously at every keyframe.
This makes the acceleration effectively infinite there.
At every keyframe, the oscillator receives a jarring ``impulse'' which distractingly changes its motion.

By contrast, the delocalized oscillators undergo perfectly smooth changes in motion.
In fact, their velocity and acceleration distributions are the same as the position distribution (up to a change of scale).

\subsection{Statistical Properties}
\label{sub:statistical_properties}

Gaussian oscillators are marginally Gaussian by definition.
However, this property applies to \textit{populations} of oscillators, not the individual oscillators themselves.

To see this point starkly, consider the simplest Gaussian Oscillator: a single Gaussian draw, constant for all time.
The marginal distribution over all such oscillators is clearly the standard Gaussian at all times.
But the marginal distribution for a \textit{single} oscillator is a point mass, which is very different from a Gaussian.

<<ks_data, include=FALSE>>=
# The number of Gaussian Oscillators of each type to construct.  Increasing
# this number gives more reliable density estimation.
num_oscillators <- 10000

# The number of equivalent independent normal draws to test.
equivalent_draws <- c(2, 10, 50)
names(equivalent_draws) <- equivalent_draws

# Utility function to compute the KS statistic on each column of a matrix.
KsOnColumns <- function(m) {
  apply(m, 2, function(v) ks.test(v, pnorm)$statistic)
}

RandomNormals <- function(n.rows, n.cols) {
  matrix(rnorm(n=n.rows * n.cols), nrow=n.rows)
}

# We want to create the desired number of copies of each oscillator.
# First, we'll do independent Gaussian draws as a reference point.
ks.independent <- lapply(equivalent_draws,
                         function(num) {
                           KsOnColumns(RandomNormals(num, num_oscillators))
                         })
# How many points to evaluate continuous timetraces at.  The higher this
# number, the more precision we have for *individual* oscillators' KS
# statistics.
num.continuous.points <- 1000
# Now we can compute the statistics for the Gaussian Oscillators (and a few
# more which aren't GOs).  Start by computing a matrix with enough normals for
# everything; we'll just take the top N rows to get N independent draws.
common.seed <- RandomNormals(max(equivalent_draws), num_oscillators)
KsForMethod <- function(matrix.function) {
  lapply(equivalent_draws,
         function(num) {
           KsOnColumns(
               matrix.function(
                   num, TimeSequence(num, num.continuous.points, loop=TRUE))
               %*% common.seed[1:num, ])
         })
}
ks.trig <- KsForMethod(InterpTrig)
ks.delocalized <- KsForMethod(DelocalizedMatrix)

ListToDataFrame <- function(L, name) {
  d <- cbind(melt(do.call(cbind.data.frame, L)), type=name)
  names(d)[which(names(d) == 'variable')] <- 'num.draws'
  names(d)[which(names(d) == 'value')] <- 'x'
  return (d)
}
ks.all <- rbind(
    ListToDataFrame(ks.independent, 'Independent Draws'),
    ListToDataFrame(ks.trig, 'Trigonometric Interpolation'),
    ListToDataFrame(ks.delocalized, 'Delocalized (this work)'))
ks.figure.caption <- sprintf(
    paste('How faithfully does each Gaussian Oscillator represent the normal',
          'distribution? This figure uses the Kolmogorov-Smirnov distribution',
          'to answer that question.  (Each curve is a histogram of %s draws.)',
          '(a) Independent draws from the normal distribution (which are not',
          'Gaussian Oscillators) provide a basis for comparison.  As expected,',
          'taking more draws shifts the distribution towards smaller values,',
          'indicating a better fit.',
          '(b), (c), and (d) show the Gaussian Oscillators for %d, %d, and %d',
          'draws, respectively, alongside the same number of independent',
          'draws.  Trigonometric interpolation is always more faithful than',
          'the same number of independent draws, but the delocalized',
          'oscillators this paper introduces are significantly better than',
          'both.'),
    formatC(num_oscillators, big.mark=',', format='fg'),
    equivalent_draws[1],
    equivalent_draws[2],
    equivalent_draws[3])
@

This motivates us to evaluate each family of Gaussian Oscillators according to how faithfully its members represent the standard Gaussian distribution.
The Kolmogorov-Smirnov (KS) statistic -- essentially, the widest vertical gap between two CDFs -- is a convenient, widely-used technique to compare two distributions.
The KS \textit{distribution} summarizes the fidelity of a Gaussian Oscillator \textit{family}: distributions which tend toward smaller values indicate more faithful families.

As a basis for comparison, consider the KS distribution for a given number $N$ of independent normal draws, without interpolation.
Larger samples represent their distributions more faithfully; therefore, we expect their KS distributions to shift towards zero.
Figure \ref{fig:ks_distribution}(a) shows the results for \Sexpr{equivalent_draws[1]}, \Sexpr{equivalent_draws[2]}, and \Sexpr{equivalent_draws[3]} draws.
It is clear that larger samples indeed have smaller KS statistics.

The next three subfigures compare two types of Gaussian Oscillator\footnote{
  We cannot include the Great Circles approach of \citep{hennig-tr-is-8}, since technically they are not Gaussian Oscillators.
  Individual dimensions are not independent; they are constrained so that the overall magnitude stays constant.
  However, each dimension is \textit{marginally} equivalent to the $N=2$ delocalized oscillators.
  Thus, the purple curve in Figure \ref{fig:ks_distribution}(b) can be taken to apply to Hennig's work, but the subsequent purple curves cannot (since Great Circles are restricted to $N=2$).
} -- trigonometric interpolation, and the delocalized oscillators this paper introduces -- to the equivalent number of independent draws.
The figure shows that interpolating between the independent draws (orange curve) more faithfully represents the normal distribution than the independent draws themselves (green curve), since the orange curve puts more mass on lower KS values.
However, the delocalized oscillators (purple curve) are significantly better than either in every case shown here.

Thus, not only is their motion more smooth and natural, but they represent the underlying distribution more faithfully.

<<ks_distribution, echo=FALSE, cache=FALSE, fig.cap=ks.figure.caption>>=
KsFigure <- function(i) {
  d <- ks.all[i, ]
  p <- (ggplot(data=d, aes(x=x, y=..density.., colour=type,
                           group=interaction(type, num.draws)))
        + geom_freqpoly(binwidth=diff(range(d$x))/60, size=line.size)
        + theme(legend.position='none')
        + scale_colour_brewer('', palette='Set2')
        + scale_x_continuous('Kolmogorov-Smirnov statistic', limits=c(0, 1))
        + scale_y_continuous('Density')
        )
}
independent.peaks <- data.frame(draws=equivalent_draws,
                                ks=c(0.5, 0.25, 0.125),
                                density=c(3.7, 5.5, 11.5))
colour.legend <- theme(legend.justification=c(1, 1),
                       legend.position=c(1, 1),
                       legend.margin=unit(0, 'mm'),
                       legend.title=element_blank())
grid.arrange(ncol=2,
             KsFigure(which(ks.all$type == 'Independent Draws'))
                 + geom_text(inherit.aes=FALSE, hjust=0, data=independent.peaks,
                             aes(label=draws, x=ks, y=density))
                 + ggtitle('(a) Independent draws'),
             KsFigure(which(ks.all$num.draws == 2))
                 + colour.legend
                 + ggtitle(sprintf('(b) %d draws', equivalent_draws[1])),
             KsFigure(which(ks.all$num.draws == 10))
                 + colour.legend
                 + ggtitle(sprintf('(c) %d draws', equivalent_draws[2])),
             KsFigure(which(ks.all$num.draws == 50))
                 + colour.legend
                 + ggtitle(sprintf('(d) %d draws', equivalent_draws[3])))
@

The distributions in Figure \ref{fig:ks_distribution}(b) have several interesting features.
To understand this advantage, consider the $N=2$ case in Figure \ref{fig:ks_distribution}(b).

Both the independent draws and the trigonometric interpolation place 50\% of their total mass on KS values above 0.5.
This is because two random normal draws have the same sign with probability 0.5.
When this happens, interpolation cannot generate values of the opposite sign, but these account for half the total mass of the standard normal.

By contrast, the delocalized oscillators can never yield KS values above 0.5.
This is because the linear combination of sine and cosine is equivalent to a sine curve with a random amplitude and phase (and the latter does not matter when we average over a whole period).
Hence, both positive and negative values are always included\footnote{The only exception is the delocalized oscillator which is identically zero at all times, but this happens with probability 0.}.

In short: the probability for the KS value to be worse than 0.5 is 0 for delocalized oscillators, and 0.5 for the alternatives.
This helps explain the considerable advantage for the former.

Delocalized oscillators retain this advantage for higher values of $N$, although the reasons why are less clear.
In any case, the basic story remains the same: interpolating between independent draws brings a marginal increase in fidelity, while delocalizing them brings a significant increase.

\subsection{Time Correlation}
\label{sub:time_correlation}

In Monte Carlo integration, autocorrelation is undesirable.
In Gaussian animations, it is desirable at short time intervals; otherwise, the motion could not be continuous.
Nevertheless, minimizing autocorrelation at longer times is still a virtue.

A useful metric is the autocorrelation at time intervals of one unit.
For interpolation, this is the mean time between independent draws; hence, we could reasonably hope for an autocorrelation near 0.

Trigonometric interpolation achieves perfectly uncorrelated results, but only for keyframes.
Between the keyframes, the same draw $\xi_i$ contributes to both values, meaning the correlation will be nonzero.
In the worst case, the two values at $\pm \frac{1}{2}$ relative to a keyframe have a correlation of $\frac{1}{2}$, which is rather high.

The autocorrelation for delocalized oscillators is
\begin{equation}
  \langle f(t) f(t + 1) \rangle = \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
  \left( \frac{2 \pi i}{N} \right) = \frac{2}{N}.
\end{equation}
Thus, delocalized oscillators are slightly anticorrelated at single-unit intervals\footnote{
This is valid at all times, since unlike interpolated animation, the delocalized oscillators are stationary.}.
For small $N$, this autocorrelation is significantly worse than interpolation, even becoming as high as -1 (perfect anticorrelation) for $N = 2$\footnote{
This anticorrelation of -1 clearly also applies to Great Circle animation.
One time unit corresponds to half the period; therefore, all values are negated.}
However, for $N > 4$, delocalized oscillators are always better than the worst case for interpolation
Higher $N$ values continue to improve the autocorrelation (and, as noted earlier, the fidelity as measured by the KS distribution).

These benefits of higher $N$-values motivate us to consider the infinite-$N$ limit.
The sum in the covariance function (Equation \ref{eqn:do_covariance}) can be converted to an integral by taking $x \equiv \frac{2i}{N}$:
\begin{align}
  \lim_{N \rightarrow \infty} \frac{2}{N} \cos \left( \frac{2 \pi i (t_2 - t_1)}{N} \right)
    &= \int\limits_0^1 \cos \left( \pi (t_2 - t_1) x \right) \, dx \\
    &= \sinc \left( \pi \left( t_2 - t_1 \right) \right),
\end{align}
where $\sinc(0) = 1$, and $\sinc(x) \equiv \sin(x) / x \forall x \ne 0$.

Thus, in the infinite limit, the autocorrelation vanishes at all intervals which are nonzero integer multiples of 1.
The disadvantage is the considerable computational expense, both in time and space (as mentioned in Section \ref{sub:connection_with_gaussian_processes}).

\section{Future Work}
\label{sec:future_work}

\subsection{Animating non-Gaussian distributions}
\label{sub:animating_non_gaussian_distributions}

This paper has shown how to visualize arbitrary Gaussian distributions using continuous animations.
The question naturally arises which other distributions can be visualized in this way.
Some distributions are clearly impossible, but several others seem tractable.

First, consider a ``nearly-Gaussian'' distribution, i.e., one whose probability density $\rho(x)$ differs from the Gaussian density $\nu(x)$ by some known factor (which naturally depends on $x$).
The time which a standard Gaussian Oscillator spends at $x$ is proportional to $\nu(x)$.
To make it proportional to $\rho(x)$ instead, we can modulate the delay before the next frame by a factor $(\rho(x) / \nu(x))$.
In essence, we are warping time to capture the subtle departures of the true distribution from the Gaussian.

A more challenging case is distributions with multiple regions that are connected only thinly (or even disconnected entirely).
Time warping seems infeasible here for several reasons.
First, the oscillator would need to move infinitely quickly in the intermodal region.
Furthermore, any single Gaussian is likely to be a very poor approximation to the distribution as a whole, so the time warping would need to be extreme.

A better alternative would be to show a separate animation for each region.
A Gaussian can better approximate individual modes rather than the entire distribution.
And the total probability mass of each mode can be indicated beside its animation.

Discrete distributions present a greater challenge: obviously, they cannot be represented by continuous animations.
However, even here there are special cases which admit partial success.
Consider a Poisson distribution (representing, e.g., neutron counts in a scattering curve\cite{hklm2010}).
The Anscombe transform\cite{anscombe1948} takes Poisson-distributed data into Gaussian-distributed to an excellent approximation.
One could animate the transformed distribution with Gaussian Oscillators, ``un-transform'' the result, and round to the nearest integer.
While the result would not be truly continuous, it would change in an ordered way which is easy for the eye to follow.

Animations are highly useful for visualizing complex distributions.
With the Gaussian case now well in hand, these other distributions beg to be explored.

\subsection{Alternative time-domain covariance functions}
\label{sub:alternative_time_domain_covariance_functions}

(Compact support: endless exploration!)

(Challenging to do in R: no framebuf support.)

((However, could output javascript with d3.js!))

\section{Conclusions}
\label{sec:conclusions}

\section{Acknowledgements}
\label{sec:acknowledgements}

\begin{itemize}
  \item yarin gal (hennig intro)
  \item NIST
  \item B. McMichael (parametric models)
\end{itemize}

\section*{Appendix: Extending Gaussian Oscillators in time}
\label{sec:appendix_extending_gaussian_oscillators_in_time}

Suppose we have a Gaussian Oscillator timetrace, consisting of $n$ points.
Can we extend the timetrace by computing the $(n + 1)$st?
If so, what is the computational cost?

This appendix will show that we \textit{can} extend Gaussian Oscillators, but the computational cost is unacceptably high.
Most Gaussian Oscillators are $O(n^2)$ in both time and space.
Oscillators with \textit{compact support} are the exception: they can be computed very efficiently ($O(1)$ with respect to $n$).
This suggests that compact support oscillators represent the future of Gaussian animations.

\subsection*{The general case}
\label{sub:the_general_case}

Let us treat the timetrace as a Gaussian process.
This means we have generated $n$ i.i.d. standard normal variates, $\xi^{(n)} \equiv \left\{\xi_1, \cdots, \xi_n\right\}$, and the lower-Cholesky decomposition $L^{(n)}$ of the covariance matrix $K^{(n)}$.
The first $n$ points are given by
\begin{equation}
  \alpha^{(n)} = L^{(n)} \xi^{(n)}.
\end{equation}

The covariance matrix $K^{(n + 1)}$ for the first $(n + 1)$ points can be written in block diagonal form:
\begin{equation}
  K^{(n + 1)} = \left[
    \begin{array}{cc}
      K^{(n)} & k^{(n)} \\
      k^{(n)\transpose} & 1
    \end{array}
    \right],
\end{equation}
where $K^{(n)}$ is the covariance matrix for the first $n$ points, and $k^{(n)}$ is a vector giving the covariance of the new point with each old point.
The Cholesky root can be similarly decomposed:
\begin{equation}
  L^{(n + 1)} = \left[
    \begin{array}{cc}
      L^{(n)} & 0 \\
      \ell^{(n)\transpose} & \gamma
    \end{array}
    \right],
\end{equation}
where $L^{(n)}$ is the Cholesky root of $K^{(n)}$, $\ell^{(n)}$ is an unknown vector, and $\gamma$ is an unknown scalar.

The $(n + 1)$st point is
\begin{equation}
  \alpha_{n + 1} = \left[
    \begin{array}{cc}
      \ell^{(n)\transpose} & \gamma
    \end{array}
  \right] \xi^{(n + 1)}.
  \label{eqn:next_point}
\end{equation}
At this point, we drop the $(n)$ and $(n + 1)$ superscripts for convenience.
The definition of the Cholesky root gives the following block matrix equation:
\begin{equation}
  \left[
    \begin{array}{cc}
      LL^\transpose & L\ell \\
      \ell^\transpose L^\transpose & \ell^\transpose \ell + \gamma^2
    \end{array}
  \right] = \left[
    \begin{array}{cc}
      K & k \\
      k^\transpose & 1
    \end{array}
  \right],
\end{equation}
which yields the vector equation
\begin{equation}
  L \ell = k
  \label{eqn:go_cov_old_new}
\end{equation}
and the scalar equation
\begin{equation}
  \ell^\transpose \ell + \gamma^2 = 1
\end{equation}
Since $L$ is lower-triangular and $k$ is known, Equation \ref{eqn:go_cov_old_new} can be trivially solved for $\ell$, and we can compute the $(n + 1)$st point.

However, the computational cost is catastrophic.
Imagine we leave the animation running, so that $n$ continues to increase.
The time to compute each new point by Equation \ref{eqn:go_cov_old_new} is $O(n^2)$.
The storage also increases as $O(n^2)$, since we require the entire matrix $L^{(n)}$.
This animation will consume all system resources at an ever-increasing pace, wihle delivering new points ever more slowly.

\subsection*{Compact Support}
\label{sub:compact_support}

To prevent this disaster, suppose the covariance matrix has compact support, so that the new value is correlated only with the previous $m$.
This means that $\left\{k_1, \cdots, k_{n - m} \right\}$ are all 0.
Equation \ref{eqn:go_cov_old_new} implies that $\left\{\ell_1, \cdots, \ell_{n - m} \right\}$ are also 0.

This yields enormous computational savings.
Since $L$ is lower-triangular, only the final $m \times m$ submatrix actually contributes to the calculation of $\ell$ (by Equation \ref{eqn:go_cov_old_new}).
Similarly, Equation \ref{eqn:next_point} only uses the $m$ most recent random variates, $\left\{\xi_{n - m + 1}, \cdots, \xi_n\right\}$; all the preceding ones can be discarded.
The important point to notice is that our computational requirements no longer grow without limit as the animation continues to run.

We can do still better.

Imagine now that the time interval between successive points is constant.
Then each row of $K$ (after the $m$th) is equivalent to the previous row, shifted by one column.
The same will be true for $L$.
Since each row has the same $m$ nonzero values, they need only be computed once.

We also only need to store $m$ random values, since the $n$th value can replace the $(n - m)$th (which is no longer needed).
The following R snippet\footnote{Note that this code is intended only to illustrate the concept; it is probably too inefficient for a real implementation.} illustrates how to generate an \texttt{n}-step timetrace, given a precomputed vector \texttt{ell} of length \texttt{m}:
<<compact_support_timetrace, echo=TRUE, eval=FALSE>>=
xi <- rnorm(m)
alpha <- c()
for (i in 1:n) {
  alpha <- c(alpha, xi %*% ell)
  xi <- c(tail(xi, -1), rnorm(1))
}
@
The code inside the \texttt{for}-loop can be repeated arbitrarily many times to extend the length of the Gaussisan Oscillator.

To summarize: extending an $n$-point Gaussian Oscillator timetrace is generally $O(n^2)$ in both time and space, which is unacceptable.
But if the Gaussian Oscillator has compact support, both costs drop to $O(m^2)$ (where $m$ is small and constant).
And if we furthermore evaluate that oscillator only at constant intervals, the costs drop further to $O(m)$.

\bibliographystyle{plain}
\bibliography{biblio}
\end{document}

Ideas which need a home

(Looping vs. non-looping)
