\documentclass[12pt,letterpaper]{article}
\usepackage{amsmath,amssymb}
\usepackage{natbib}

\DeclareMathOperator{\sinc}{sinc}
\newcommand{\transpose}{\intercal}

\begin{document}

\title{Visualizing uncertain curves and surfaces via Gaussian Oscillators}
\author{Charles~R.~Hogg~III}
\date{August~2014}
\maketitle

<<global_setup, cache=FALSE, include=FALSE>>=
require(ggplot2)
require(knitr)
require(gridExtra)

source('interpolation.R')

# "Draft mode" makes sketch-y figures with low statistics.
draft_mode <- TRUE

# Smart caching.
opts_chunk$set(cache=TRUE, autodep=TRUE)

# ggplot2 options.
theme_set(theme_grey(base_size=12))
line.size <- 1

# For reproducibility.
set.seed(1)
@

\section{Introduction}
\label{sec:introduction}

Curves and surfaces abound in the sciences.
It is vital to quantify their uncertainty -- usually, by computing a probability distribution.
It is equally vital to visualize that uncertainty.
Here, curves and surfaces present considerable challenges.

The most common technique is to plot upper and lower credible intervals along with the estimated curve.
This useful technique conveys considerable information at a glance, but there is some information it cannot convey.
Since intervals are pointwise, they tell us nothing about correlations between points.
In other words, we know the curve lies roughly within a boundary, but we don't know what it's doing within that boundary.

To convey this missing information, we need a complementary technique which depicts individual members of the distribution.
The simplest such approach samples several curves from the distribution and plots them together.
Unfortunately, there is a fundamental tension: showing too few curves will not fully represent the distribution, while showing too many quickly clutters the figure.

Animations resolve this tension by introducing a time dimension: we show as many curves as necessary, but only one at a time.
If each frame is correlated with its neighbours, the animation becomes \textit{continuous}, and the curve appears as a quasi-physical moving object.
The eye is naturally drawn to areas with more motion, which represent higher uncertainty.
Thus, besides their aesthetic appeal, continuous animations are an easily interpretable method to convey uncertainty in curves.

Naturally, all these considerations apply to uncertain \textit{surfaces} too, but with greater force.
Credible intervals are difficult to depict accurately because surfaces obscure one another.
Animations are much better, because they never show more than one surface at a time.

This paper focuses on curves and surfaces whose underlying distribution is Gaussian.
This is not nearly as restrictive as it might seem.
It includes Gaussian Processes\cite{3569}, a flexible nonparametric class of models.
Furthermore, parametric models often have a single posterior mode, which is Gaussian to leading order.
Continuous paths through the Gaussian parameter space correspond to continuous animations of the function those parameters govern.

The present author is aware of two main approaches in the literature.

Historically, most animations have been based on \textit{interpolation}\cite{skilling1992,esg1997}.
A series of independent draws from the Gaussian are generated and displayed at regular time intervals; these are called the \textit{keyframes}.
The frames at intervening times are weighted averages of the nearest two keyframes.
The weighting favours the first keyframe at the beginning of the interval, and the second keyframe near the end, so that the curve or surface changes continuously from one keyframe to the next.
Figure \ref{fig:interpolation}(a) illustrates the idea for a simple 1-dimensional Gaussian using three possible interpolating families: \textit{linear}, \textit{cubic spline}, and \textit{trigonometric}.

<<interpolation_plots, include=FALSE>>=
quantiles <- c(0.1, 0.5, 0.9)
num.interpolated.curves <- ifelse(draft_mode, 1e3, 1e6)
num.points <- 500
num.keyframes <- 4
interpolation.caption <- sprintf(
    paste("Interpolation illustrated for a standard 1-D Gaussian.",
          "(a) A single set of normal draws interpolated by three methods.",
          "(b) Pointwise quantiles for interpolating %s random draws."),
    formatC(num.interpolated.curves, big.mark=',', format='fg'))

interp.matrix.functions <- list(Linear=InterpLinear,
                                Spline=InterpSpline,
                                Trigonometric=InterpTrig)

p.concept <- InterpolationConceptPlot(
    interp.matrix.functions=interp.matrix.functions,
    n.points=num.points,
    n.keyframes=num.keyframes,
    size=line.size)
p.interp <- InterpolationQuantilePlot(
    interp.matrix.functions=interp.matrix.functions,
    quantiles=quantiles,
    n.draws=num.interpolated.curves,
    n.points=num.points,
    n.keyframes=num.keyframes,
    size=line.size)
@

<<interpolation, cache=FALSE, echo=FALSE, fig.cap=interpolation.caption>>=
grid.arrange(p.concept + scale_colour_brewer('', palette='Set2'),
             p.interp + scale_colour_brewer('', palette='Set2'),
             ncol=1)
@

Unfortunately, interpolation can easily yield incorrect statistics between the keyframes.
In particular, the variance is usually underestimated.
Figure \ref{fig:interpolation}(b) shows that only the trigonometric approach\cite{esg1997,skilling1992} correctly preserves statistical properties between keyframes.

An important alternative was pioneered by Hennig\cite{hennig-tr-is-8}, who generated great-circle orbits through parameter space with marginal normal distributions.
The starting point is drawn randomly from the multivariate Gaussian.
The path begins in the direction of a second randomly drawn point, constrained to a constant distance from the origin.
Since no point on a circle is special, every frame in these circular timetraces is fully equivalent to every other.
To the best of the author's knowledge, Hennig's paper is the first published example of a smooth, keyframe-free Gaussian animation.

Each approach has advantages and weaknesses.
Interpolation is flexible: it can incorporate arbitrarily many independent draws, yielding paths which better represent the Normal distribution.
The downside is that not all frames are equivalent: keyframes have preferred status, and the motion changes discontinuously there.

Great Circle animations\cite{hennig-tr-is-8} do not have this defect; they treat all frames equally in every way.
They also represent the normal distribution more faithfully than interpolated animations whenever the latter also use two independent draws.
Their shortcoming is that they cannot incorporate additional independent draws, and therefore cannot improve their fidelity.

This paper proposes a concept to unify these and future approaches: the \textit{Gaussian oscillator}.
We then introduce a novel Gaussian oscillator technique, which combines the extensibility of interpolation with the smooth, natural motion of the great circles.
We develop quality metrics to assess the strengths and weaknesses of each method.
Finally, we suggest how to leverage the Gaussian oscillator framework to construct even better animations in the future.

\section{Gaussian Oscillators}
\label{sec:gaussian_oscillators}

This section will define and explain the \textit{Gaussian Oscillator}: the key concept for analyzing Gaussian animations.

First, consider a $m$-dimensional Gaussian, with mean vector $\mu$ and covariance matrix $K$.
To take a single draw from this distribution, we need the lower-triangular Cholesky decomposition $L$ of $K$, which satisfies $LL^\intercal = K$.
If $\xi = \left( \xi_1, \cdots, \xi_m \right)^\intercal$ represents $m$ i.i.d. draws from the \textit{standard} normal $N(0, 1)$, then $\alpha \equiv \mu + L\xi$ is a draw from $N(\mu, K)$.

Now imagine that each $\xi_i$ is a random function of time, $\xi_i(t)$, such that $\xi_i(t) \sim N(0, 1) \,\, \forall t$.
It follows that $\alpha(t) \sim N(\mu, K) \,\, \forall t$.
Thus, the problem of animating arbitrary multidimensional Gaussian distributions reduces to that of animating a single-dimensional standard Gaussian.
This latter problem is the focus of the rest of the paper.

This motivates us to define a \textit{Gaussian Oscillator} as a stochastic process, $\xi(t)$, with the following properties.
\begin{equation}
  \xi(t) \sim N(0, 1) \,\, \forall t
  \label{eqn:go_marginal}
\end{equation}
\begin{equation}
  \lim_{\Delta t \rightarrow 0} \langle \xi(t) \xi(t + \Delta t) \rangle = 1 \,\, \forall t
  \label{eqn:go_continuous}
\end{equation}
Equation \ref{eqn:go_marginal} means that every frame of the animations is marginally Gaussian.
Equation \ref{eqn:go_continuous} means that the animation is \textit{continuous}.

Most $m$-dimensional Gaussian animations can be analyzed by combining $m$ independent Gaussian oscillators.
Great Circle animations are an exception, since they sample directly from the $m$-dimensional distribution.
Each individual dimension of a Great Circle animation fulfills the definition of a Gaussian Oscillator.
However, the oscillators are not independent (for example, the $m$-dimensional vector is constrained to have constant magnitude).

\subsection{Connection with Gaussian Processes}
\label{sub:connection_with_gaussian_processes}

Readers familiar with the concept will have noticed that Gaussian Oscillators are also Gaussian \textit{Processes}\cite{3569}, with a few constraints.
Specifically, the mean function $\mu(t)$ is zero everywhere, and the covariance function $k(t, t')$ is 1 when $t = t'$ (and continuous in that neighbourhood).

Remarkably, nobody seems to have pointed out this connection before.
It is extremely useful, since future Gaussian animation research can leverage the well-studied menagerie of covariance functions\cite{3569}.

There is some danger of confusion here, because often the curves and surfaces being animated are also modeled using Gaussian Processes.
The difference is that the Gaussian Oscillator is a \textit{time-}domain Gaussian Process, while the curve or surface lives in the \textit{space} domain\footnote{
The ``space'' here might be an abstract space.  It might even be (physical) time --- for example, with time-series analysis.  However, the point is that this is distinct from the ``animation time'' domain where the Gaussian Oscillator lives.
}.

Treating Gaussian Oscillators as Gaussian Processes is likely to be more useful for theoretical analysis than for practical computations\footnote{
  Gaussian Processes with compact support --- discussed in the appendix --- are a notable exception.
}.
Consider a trigonometric interpolation of $k$ keyframes and $n$ total frames; typically, $n \gg k$.
Computing $\xi(t)$ as a Gaussian Process requires $n$ i.i.d. normal draws, and an $\mathcal{O}(n^3)$ matrix inversion!
By contrast, explicit interpolation requires only $k$ normal draws (and simple sines and cosines instead of the matrix inversion).

\section{A New Way to Generate Animations}
\label{sec:a_new_way_to_generate_animations}

The new animations in this paper are most naturally compared to the interpolation technique.
Both techniques involve generating some number $N$ of i.i.d. standard normal variates, then using them deterministically to produce a continuous timetrace.
With interpolation, each variate influences only the local region between its nearest neighbours.
In this paper's technique, each variate's influence is spread out across the entire timetrace.

Explicitly, given $N$ i.i.d. normal variates $\{\epsilon_1, \cdots, \epsilon_n\}$, the corresponding \textit{delocalized oscillator} $\xi(t)$ is given by
\begin{equation}
  \label{eqn:delocalized}
  \xi(t) = \sqrt{\frac{2}{N}} \sum\limits_{i=1}^{N/2} \left[
  \epsilon_{2i-1} \sin \left( \frac{2 \pi i t}{N} \right) +
  \epsilon_{2i} \cos \left( \frac{2 \pi i t}{N} \right)
\right].
\end{equation}
Note that this is only defined for even $N$, since every $\sin(\cdot)$ needs a corresponding $\cos(\cdot)$.
Considering the application, this restriction seems mild.

Equation \ref{eqn:delocalized} superficially resembles a Fourier series, in that it is a weighted sum of sines and cosines.

To verify that $\xi(t)$ is a Gaussian Oscillator, the relations $\langle \epsilon_i \rangle = 0$ and $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$ will be useful.
First, note that $\xi(t)$ is a sum of Gaussian random variables at each time $t$; therefore, $\xi(t)$ also has a Gaussian distribution.
This means that its mean $\langle \xi(t) \rangle$ and covariance $\langle \xi(t_1) \xi(t_2) \rangle$ characterize it completely.

From the definition in Equation \ref{eqn:delocalized}, and the property $\langle \epsilon_i \rangle = 0$, it is clear that $\langle \xi(t) \rangle = 0$.
The covariance requires more work.
It involves a double sum, but since $\langle \epsilon_i \epsilon_j \rangle = \delta_{ij}$, only the $i=j$ terms survive:
\begin{align}
  \langle \xi(t_1) \xi(t_2) \rangle &= \frac{2}{N} \sum\limits_{i=1}^{N/2}
    \left[
      \sin \left( \frac{2 \pi i t_1}{N} \right) \sin \left( \frac{2 \pi i t_2}{N} \right) +
      \cos \left( \frac{2 \pi i t_1}{N} \right) \cos \left( \frac{2 \pi i t_2}{N} \right)
    \right] \\
  &= \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
    \left( \frac{2 \pi i (t_2 - t_1)}{N} \right).
    \label{eqn:do_covariance}
\end{align}
Note that this depends only on $(t_2 - t_1)$: these Gaussian Oscillators are \textit{stationary}, so no keyframes are singled out.
Note too that $\langle \xi(t)^2 \rangle = 1 \,\,\,\, \forall t$, as required for Gaussian Oscillators.

\section{Comparison}
\label{sec:comparison}

This section introduces metrics to judge the quality of each type of Gaussian Oscillator.
These metrics assume that the animation is composed of independent Gaussian Oscillators, as is usually the case.
Great Circle animations\cite{hennig-tr-is-8} violate this assumption, so we must begin by clarifying their role.

Great Circle animations are very similar to the $N = 2$ case of delocalized oscillators.
Both techniques require $2d$ random variables to visualize a $d$-dimensional Gaussian, and both techniques produce orbits which are confined to a random plane.
In fact, Great Circle animations are a subset of measure 0 of the $N = 2$ delocalized oscillators: the former always produces circles, but the latter produces ellipses.
Therefore, we take the results for $N = 2$ delocalized oscillators as a proxy for the quality of the Great Circle animations.

\subsection{Basis functions}
\label{sub:basis_functions}

<<common_prep, include=FALSE>>=
# Parameters to govern the output for the basis and kinematics plots.
N.out <- 400
N.frames <- 4
N.draws <- 20
t.out <- seq(from=0, to=N.frames, length=N.out)

weight <- function(t.in, t.out, FUN) {
  ifelse(abs(t.in - t.out) > 1, 0, FUN(t.in, t.out))
}
periodic_weight <- function(t.in, t.out, period, FUN) {
  # Hack to make the basis functions periodic (instead of always 0 at t=0).
  pmax(weight(t.in, t.out, FUN), weight(t.in, t.out - period, FUN))
}

DelocalizedMatrix <- function(N.frames, t) {
  N <- N.frames / 2
  cbind(
    outer(t, 1:N, function(t, i) sin(pi * i * t / N)),
    outer(t, N:1, function(t, i) cos(pi * i * t / N))) / sqrt(N)
}
InterpolationMatrixESG <- function(N.frames, t.out) {
  t.frames <- 1:N.frames
  outer(t.out, t.frames, 
    function(x, y) {
      periodic_weight(x, y, N.frames, function(a, b) cos((a - b) * pi / 2))
    })
}

# Matrices.
interp.esg <- InterpolationMatrixESG(N.frames, t.out)
interp.deloc <- DelocalizedMatrix(N.frames, t.out)
@

Both interpolation and delocalized oscillators can be viewed as a linear combination of basis functions with random coefficients.
Comparing the basis functions for each method reveals the key differences between them.
Figure \ref{fig:basis} shows these basis functions.

<<basis_prep, include=FALSE>>=
basis.colors <- scale_colour_brewer(palette='Set2')
basis.theme <- theme(legend.position='none')
basis.plot <- function(mat, title) {
  d <- data.frame(y=as.vector(mat), t=t.out,
    label=rep(paste0('a', 1:N.frames), each=N.out))
  p <- (ggplot(data=d, aes(x=t, y=y, colour=label))
    + geom_hline(yintercept=0)
    + geom_line(size=1)
    + scale_x_continuous('', limits=c(0, N.frames))
    + scale_y_continuous('', limits=c(-1, 1), breaks=(-1):1)
    + basis.colors
    + basis.theme
    + ggtitle(title)
    + theme(axis.text.x=element_blank())
    )
  return (p)
}
basis.caption <- sprintf(
  paste(
    'Basis functions for two types of Gaussian Oscillators (with %d independent draws).'),
  N.frames
  )
@

<<basis, echo=FALSE, fig.cap=basis.caption, fig.height=4>>=
grid.arrange(ncol=1,
  basis.plot(interp.esg, 'a) Trigonometric basis functions'),
  basis.plot(interp.deloc, 'b) Delocalized basis functions'))
@

(Discuss them.)

\subsection{Kinematic Properties}
\label{sub:kinematic_properties}

One way to compare Gaussian Oscillators is to compute their most basic kinematic properties: velocity and acceleration.
These distributions capture information which eludes the marginal \textit{position} distribution (which is the same by definition for all Gaussian Oscillators).

<<kinematics_prep, include=FALSE>>=
approximate_derivative <- function(mat, t, deriv=0) {
  # Take the derivative with respect to the rows.
  if (deriv == 0) {
    return (mat)
  }
  mat.aug <- rbind(mat, mat[1, ])
  t.aug <- c(t, t[1])
  approximate_derivative(diff(mat.aug) / diff(t.aug), t, deriv - 1)
}

graph <- function(label, interp.mat, deriv=0, scale=1, N.draws=3, seed=2,
                  show_keyframes=FALSE) {
  set.seed(seed)
  mat <- approximate_derivative(interp.mat, t=t.out, deriv=deriv)
  random.data <- matrix(rnorm(n=ncol(mat) * N.draws), ncol=N.draws)
  timetraces <- data.frame(y=as.vector(mat %*% random.data),
                           group=rep(1:N.draws, each=nrow(mat)),
                           t=t.out)
  first_timetrace <- timetraces[1:nrow(mat), ]
  p <- (ggplot(data=timetraces, aes(x=t, y=y, group=group))
        + geom_line(colour='#999999')
        + geom_line(data=first_timetrace)
        + scale_y_continuous("", breaks=scale * (-3):3)
        + scale_x_continuous("")
        + coord_cartesian(ylim=3 * ((pi / 2) ^ (2 * deriv)) * c(-1, 1))
        + theme(axis.text.x=element_blank())
        + theme(plot.margin=unit(rep(0.1, 4), "cm"))
        )
  if (show_keyframes) {
    p <- p + geom_point(data=data.frame(t=0:nrow(random.data),
                                        y=c(tail(random.data[, 1], 1),
                                            random.data[, 1])),
                        aes(x=t, y=y), inherit.aes=FALSE)
  }
  return (p)
}

graphs <- function(label, mat, n) {
  interpolating <- length(grep('Interpolation', label)) > 0
  list(graph(label, mat, N.draws=n, deriv=0, scale=2, show_keyframes=interpolating),
       graph(label, mat, N.draws=n, deriv=1, scale=5),
       graph(label, mat, N.draws=n, deriv=2, scale=10))
}

# Construct the graphs.
graphs.esg <- graphs(label="ESG Interpolation", mat=interp.esg, n=N.draws)
graphs.deloc <- graphs(label="Smooth Timetraces", mat=interp.deloc, n=N.draws)
quickie.text <- function(x, ...) textGrob(x, just='center', gp=gpar(fontsize=15), ...)
kinematics.caption <- sprintf(
  paste(
    'The basic kinematics of two types of Gaussian Oscillators:',
    'the standard trigonometric oscillators, and the ``delocalized oscillators\'\' this paper introduces.',
    'Both populations are illustrated with %d draws (grey), one of which is randomly chosen and highlighted for illustration.',
    'The velocity and acceleration graphs for the Trigonometric oscillators clearly show that keyframes are treated specially.',
    'By contrast, all times for the Delocalized oscillators are statistically equivalent.'),
  N.draws
  )
@

<<kinematics, cache=FALSE, echo=FALSE, fig.cap=kinematics.caption, fig.height=4>>=
grid.arrange(nullGrob(),
             quickie.text('Position'),
             quickie.text('Velocity'),
             quickie.text('Acceleration'),
             quickie.text('Trigonometric', rot=90), graphs.esg[[1]], graphs.esg[[2]], graphs.esg[[3]], 
             quickie.text('Delocalized\n(this paper)', rot=90), graphs.deloc[[1]], graphs.deloc[[2]], graphs.deloc[[3]],
             ncol=4,
             heights=c(0.2, 1, 1, 1),
             widths=c(0.23, 1, 1, 1))
@
Figure \ref{fig:kinematics} shows the results.
The velocity distribution for interpolation changes discontinuously at every keyframe.
This makes the acceleration effectively infinite there.
At every keyframe, the oscillator receives an ``impulse,'' which distractingly changes its motion.
By contrast, the delocalized oscillators' motion is perfectly smooth.
Each time is statistically equivalent to each other time --- not just for the oscillator's position, but for its velocity and acceleration too.
The keyframes are not special because they do not exist.

To understand these different behaviours, we view these oscillators as Gaussian Processes, and compare their covariance functions $k(t_1, t_2)$.
Delocalized oscillators have a \textit{stationary} covariance (Equation \ref{eqn:do_covariance}): it depends only on $(t_2 - t_1)$, not on $t_1$ or $t_2$ individually.
Trigonometric interpolation has a non-stationary covariance:
\begin{equation}
  k_\text{TI}(t_1, t_2) = \sum_{i=1}^N b_i(t_1) b_i(t_2),
\end{equation}
where $b_i(t)$ is the $i$th basis function (see Figure \ref{fig:basis}).\footnote{
  Note that the covariance from one keyframe to the next (e.g., $k_\text{TI}(0, 1)$) is 0, as we would expect.
  But the covariance \textit{between} keyframes can be quite high, even for the same interval, e.g., $k_\text{TI}(0.5, 1.5) = \frac{1}{2}$.
}

To avoid any frames being treated specially, we should prefer Gaussian Oscillators with stationary covariance.
If we further want motion that is smooth and natural, we should choose a covariance which is continuous and differentiable at $t_1 = t_2$.

\subsection{Statistical Properties}
\label{sub:statistical_properties}

Gaussian oscillators are marginally Gaussian by definition.
However, this property applies to \textit{populations} of oscillators, not the individual oscillators themselves.

To see this point starkly, consider the simplest Gaussian Oscillator: a single Gaussian draw, constant for all time.
The marginal distribution over all such oscillators is clearly the standard Gaussian at all times.
But the marginal distribution for a \textit{single} oscillator is a point mass, which is very different from a Gaussian.

<<ks_data, include=FALSE>>=
# The number of Gaussian Oscillators of each type to construct.  Increasing
# this number gives more reliable density estimation.
num_oscillators <- ifelse(draft_mode, 0.5e3, 1e6)

# The number of equivalent independent normal draws to test.
equivalent_draws <- c(2, 10, 50)
names(equivalent_draws) <- equivalent_draws

# Utility function to compute the KS statistic on each column of a matrix.
KsOnColumns <- function(m) {
  apply(m, 2, function(v) ks.test(v, pnorm)$statistic)
}

RandomNormals <- function(n.rows, n.cols) {
  matrix(rnorm(n=n.rows * n.cols), nrow=n.rows)
}

# We want to create the desired number of copies of each oscillator.
# First, we'll do independent Gaussian draws as a reference point.
ks.independent <- lapply(equivalent_draws,
                         function(num) {
                           KsOnColumns(RandomNormals(num, num_oscillators))
                         })
# How many points to evaluate continuous timetraces at.  The higher this
# number, the more precision we have for *individual* oscillators' KS
# statistics.
num.continuous.points <- 1000
# Now we can compute the statistics for the Gaussian Oscillators (and a few
# more which aren't GOs).  Start by computing a matrix with enough normals for
# everything; we'll just take the top N rows to get N independent draws.
common.seed <- RandomNormals(max(equivalent_draws), num_oscillators)
KsForMethod <- function(matrix.function) {
  lapply(equivalent_draws,
         function(num) {
           KsOnColumns(
               matrix.function(
                   num, TimeSequence(num, num.continuous.points, loop=TRUE))
               %*% common.seed[1:num, ])
         })
}
ks.trig <- KsForMethod(InterpTrig)
ks.delocalized <- KsForMethod(DelocalizedMatrix)

ListToDataFrame <- function(L, name) {
  d <- cbind(melt(do.call(cbind.data.frame, L)), type=name)
  names(d)[which(names(d) == 'variable')] <- 'num.draws'
  names(d)[which(names(d) == 'value')] <- 'x'
  return (d)
}
ks.all <- rbind(
    ListToDataFrame(ks.independent, 'Independent Draws'),
    ListToDataFrame(ks.trig, 'Trigonometric Interpolation'),
    ListToDataFrame(ks.delocalized, 'Delocalized (this work)'))
ks.figure.caption <- sprintf(
    paste('How faithfully does each Gaussian Oscillator represent the normal',
          'distribution? This figure uses the Kolmogorov-Smirnov distribution',
          'to answer that question.  (Each curve is a histogram of %s draws.)',
          '(a) Independent draws from the normal distribution (which are not',
          'Gaussian Oscillators) provide a basis for comparison.  As expected,',
          'taking more draws shifts the distribution towards smaller values,',
          'indicating a better fit.',
          '(b), (c), and (d) show the Gaussian Oscillators for %d, %d, and %d',
          'draws, respectively, alongside the same number of independent',
          'draws.  Trigonometric interpolation is always more faithful than',
          'the same number of independent draws, but the delocalized',
          'oscillators this paper introduces are significantly better than',
          'both.'),
    formatC(num_oscillators, big.mark=',', format='fg'),
    equivalent_draws[1],
    equivalent_draws[2],
    equivalent_draws[3])
@

This motivates us to evaluate each family of Gaussian Oscillators according to how faithfully its members represent the standard Gaussian distribution.
The Kolmogorov-Smirnov (KS) statistic -- essentially, the widest vertical gap between two CDFs -- is a convenient, widely-used technique to compare two distributions.
The KS \textit{distribution} summarizes the fidelity of a Gaussian Oscillator \textit{family}: distributions which tend toward smaller values indicate more faithful families.\footnote{
  Of course, the same idea could be applied to any other metric which compares probability distributions.
}

As a basis for comparison, consider the KS distribution for a given number $N$ of independent normal draws, without interpolation.\footnote{
  To be clear: these are not Gaussian Oscillators; they are merely a basis for comparison.
}
Larger samples represent their distributions more faithfully; therefore, we expect their KS distributions to shift towards zero.
Figure \ref{fig:ks_distribution}(a) shows the results for \Sexpr{equivalent_draws[1]}, \Sexpr{equivalent_draws[2]}, and \Sexpr{equivalent_draws[3]} draws.
It is clear that larger samples indeed have smaller KS statistics.

The next three subfigures compare two types of Gaussian Oscillator\footnote{
  We cannot include the Great Circles approach of \citep{hennig-tr-is-8}, since technically they are not Gaussian Oscillators.
  Individual dimensions are not independent; they are constrained so that the overall magnitude stays constant.
  However, each dimension is \textit{marginally} equivalent to the $N=2$ delocalized oscillators.
  Thus, the purple curve in Figure \ref{fig:ks_distribution}(b) can be taken to apply to Hennig's work, but the subsequent purple curves cannot (since Great Circles are restricted to $N=2$).
} -- trigonometric interpolation, and the delocalized oscillators this paper introduces -- to the equivalent number of independent draws.
The figure shows that interpolating between the independent draws (orange curve) more faithfully represents the normal distribution than the independent draws themselves (green curve), since the orange curve puts more mass on lower KS values.
However, the delocalized oscillators (purple curve) are significantly better than either in every case shown here.

Thus, not only is their motion more smooth and natural, but they represent the underlying distribution more faithfully.

<<ks_distribution, echo=FALSE, cache=FALSE, fig.cap=ks.figure.caption>>=
KsFigure <- function(i) {
  d <- ks.all[i, ]
  p <- (ggplot(data=d, aes(x=x, y=..density.., colour=type,
                           group=interaction(type, num.draws)))
        + geom_freqpoly(binwidth=diff(range(d$x))/60, size=line.size)
        + theme(legend.position='none')
        + scale_colour_brewer('', palette='Set2')
        + scale_x_continuous('Kolmogorov-Smirnov statistic', limits=c(0, 1))
        + scale_y_continuous('Density')
        )
}
independent.peaks <- data.frame(draws=equivalent_draws,
                                ks=c(0.5, 0.25, 0.125),
                                density=c(3.7, 5.5, 11.5))
colour.legend <- theme(legend.justification=c(1, 1),
                       legend.position=c(1, 1),
                       legend.margin=unit(0, 'mm'),
                       legend.title=element_blank())
suppressWarnings(grid.arrange(
    ncol=2,
    KsFigure(which(ks.all$type == 'Independent Draws'))
       + geom_text(inherit.aes=FALSE, hjust=0, data=independent.peaks,
                   aes(label=draws, x=ks, y=density))
       + ggtitle('(a) Independent draws'),
    KsFigure(which(ks.all$num.draws == 2))
       + colour.legend
       + ggtitle(sprintf('(b) %d draws', equivalent_draws[1])),
    KsFigure(which(ks.all$num.draws == 10))
       + colour.legend
       + ggtitle(sprintf('(c) %d draws', equivalent_draws[2])),
    KsFigure(which(ks.all$num.draws == 50))
       + colour.legend
       + ggtitle(sprintf('(d) %d draws', equivalent_draws[3]))))
@

\subsection{Time Correlation}
\label{sub:time_correlation}

In Monte Carlo simulations, autocorrelation is undesirable.
In Gaussian animations, it is desirable at short time intervals; otherwise, the motion could not be continuous.
Nevertheless, minimizing autocorrelation at longer times is still a virtue.

A useful metric is the autocorrelation at time intervals of one unit.
For interpolation, this is the mean time between independent draws; hence, we could reasonably hope for a unit-time autocorrelation near 0.

Trigonometric interpolation achieves perfectly uncorrelated results, but only for keyframes.
Between the keyframes, the same draw $\varepsilon_i$ contributes to both values, meaning the correlation will be nonzero.
In the worst case, the two values at $\Delta t = \pm \frac{1}{2}$ relative to a keyframe have a correlation of $\frac{1}{2}$, which is rather high.

The unit-time autocorrelation for delocalized oscillators is
\begin{equation}
  \langle \xi(t) \xi(t + 1) \rangle = \frac{2}{N} \sum\limits_{i=1}^{N/2} \cos
  \left( \frac{2 \pi i}{N} \right) = \frac{2}{N}.
\end{equation}
Thus, delocalized oscillators are slightly anticorrelated at single-unit intervals\footnote{
This is valid at all times, since unlike interpolated animation, the delocalized oscillators are stationary.}.
For small $N$, this autocorrelation is significantly worse than interpolation, even becoming as high as -1 (perfect anticorrelation) for the $N = 2$ case.\footnote{
  This anticorrelation of -1 clearly also applies to Great Circle animation.
  One time unit corresponds to half the period; therefore, all values are negated.
}
However, for $N > 4$, delocalized oscillators are always better than the worst case for interpolation.
Higher $N$ values continue to improve the autocorrelation (and, as noted earlier, the fidelity as measured by the KS distribution).

These benefits of higher $N$-values motivate us to consider the infinite-$N$ limit.
The sum in the covariance function (Equation \ref{eqn:do_covariance}) can be converted to an integral by taking $x \equiv \frac{2i}{N}$:
\begin{align}
  \lim_{N \rightarrow \infty} \frac{2}{N} \sum_{i=1}^{N/2} \cos \left( \frac{2 \pi i (t_2 - t_1)}{N} \right)
    &= \int\limits_0^1 \cos \left( \pi (t_2 - t_1) x \right) \, dx \\
    &= \sinc \left( \pi \left( t_2 - t_1 \right) \right),
\end{align}
where $\sinc(0) = 1$, and $\sinc(x) \equiv \sin(x) / x \,\,\,\, \forall x \ne 0$.

Thus, in the infinite limit, the autocorrelation vanishes at all intervals which are nonzero integer multiples of 1.
The disadvantage is the considerable computational expense, both in time and space (as mentioned in Section \ref{sub:connection_with_gaussian_processes}).

\section{Future Work}
\label{sec:future_work}

\subsection{Animating non-Gaussian distributions}
\label{sub:animating_non_gaussian_distributions}

This paper has shown how to visualize arbitrary Gaussian distributions using continuous animations.
The question naturally arises which other distributions can be visualized in this way.
Some distributions are clearly impossible, but several others seem tractable.

First, consider a ``nearly-Gaussian'' distribution, i.e., one whose probability density $\rho(x)$ differs from the Gaussian density $\nu(x)$ by some known factor (which naturally depends on $x$).
The time which a standard Gaussian Oscillator spends at $x$ is proportional to $\nu(x)$.
To make it proportional to $\rho(x)$ instead, we can modulate the delay before the next frame by a factor $(\rho(x) / \nu(x))$.
In essence, we are warping time to capture the subtle departures of the true distribution from the Gaussian.

A more challenging case is distributions with multiple regions that are connected only thinly (or even disconnected entirely).
Time warping seems infeasible here for several reasons.
First, the oscillator would need to move infinitely quickly in the intermodal region.
Furthermore, any single Gaussian is likely to be a very poor approximation to the distribution as a whole, so the time warping would need to be extreme.

A better alternative would be to show a separate animation for each region.
A Gaussian can better approximate individual modes rather than the entire distribution.
And the total probability mass of each mode can be indicated beside its animation.

Discrete distributions present a greater challenge: obviously, they cannot be represented by continuous animations.
However, even here there are special cases which admit partial success.
Consider a Poisson distribution (representing, e.g., neutron counts in a scattering curve\cite{hklm2010}).
The Anscombe transform\cite{anscombe1948} takes Poisson-distributed data into Gaussian-distributed to an excellent approximation.
One could animate the transformed distribution with Gaussian Oscillators, ``un-transform'' the result, and round to the nearest integer.
While the result would not be truly continuous, it would change in an ordered way which is easy for the eye to follow.

Animations are highly useful for visualizing complex distributions.
With the Gaussian case now well in hand, these other distributions beg to be explored.

\subsection{Alternative time-domain covariance functions}
\label{sub:alternative_time_domain_covariance_functions}

Let us consider the quality metrics outlined in Section \ref{sec:comparison}, and try to engineer a superior Gaussian Oscillator.

First, we want every individual oscillator to represent the normal distribution faithfully (as measured by the KS distribution from Section \ref{sub:statistical_properties}).
The best possible way to achieve this would be an endless, non-looping animation, which continuously incorporates new Gaussian draws.
Gaussian Processes cannot generally accomplish this with finite resources, since every new value requires knowledge of all previous values.
However, certain special types of Gaussian Processes can make it work.
For instance, interpolated oscillators can discard all previous values.

Second, we want the motion to be smooth and natural, with all frames exactly equivalent to one another.
Interpolation fails here, since it singles out ``keyframes'' for special treatment.
To fulfill this requirement, we should focus on covariance functions which are stationary and smooth (as with the delocalized oscillators).

Finally, we want autocorrelation to vanish at all intervals longer than one unit.
Both trigonometric interpolation and delocalized oscillators fall short of this goal.
Trigonometric interpolation achieves it only for keyframes, while delocalized oscillators exhibit damped autocorrelation oscillations at longer times (Figure \ref{fig:covmatrices}).

There exists a class of covariance covariance functions which can satisfy all these constraints simultaneously: \textit{smooth, compact support}.
For example, consider this piecewise polynomial covariance function:
\begin{equation}
  k(\tau) =
  \begin{cases}
    (1 - \tau)^6(12.8 \tau^3 + 13.8 \tau^2 + 6 \tau + 1) & 0 \le \tau \le 1 \\
    0 & \tau > 1,
  \end{cases}
\end{equation}
where $\tau$ is the time interval magnitude $|t_2 - t_1|$.
Curves sampled from this distribution are twice differentiable everywhere; hence, their motion will be smooth and natural.
Their autocorrelation completely vanishes at all intervals longer than one unit: the best possible result.
The covariance is stationary, so no frames can be special in any way.
Finally, they permit infinite non-repeating animations with finite resources, just as interpolation does\footnote{
  The appendix explains in detail how to do this.
}.

Considering these advantages, smooth covariance functions with compact support seem very difficult to beat.
The challenge is to build software which supports infinite non-repeating animations in a format which can be widely shared.
HTML/CSS/javascript is a promising target, since it is widely available and highly cross-platform.
The \texttt{R} programming language\cite{rlang} already supports packages which can output analysis to HTML --- for example, \texttt{shiny}\cite{shiny}.
Such a package could be augmented to include javascript which continually updates the points' coordinates, based on newly generated random variables.

This, then, is the bright future we envision for animated uncertainty visualization.
The uncertainty can be analyzed in \texttt{R}, as usual --- but rather than a static image or animated gif, the output is an interactive web app.
The curve continuously moves in a smooth and natural way, never repeating its position, visualizing more and more of the curve distribution which the analysis computed.

\section{Conclusions}
\label{sec:conclusions}

\section{Acknowledgements}
\label{sec:acknowledgements}

I gratefully acknowledge the help and support which made this work possible.
First, I am thankful to the National Institute of Standards and Technology, where I was employed when I initially developed delocalized oscillators.
Bob McMichael greatly expanded the utility of this work by pointing out that it applies to parametric models, not just Gaussian Processes.
Finally, Yarin Gal introduced me to the delightful and mesmerizing animations of Philipp Hennig.

\section*{Appendix: Extending Gaussian Oscillators in time}
\label{sec:appendix_extending_gaussian_oscillators_in_time}

Suppose we have a Gaussian Oscillator timetrace, consisting of $n$ points.
Can we extend the timetrace by computing the $(n + 1)$st?
If so, what is the computational cost?

This appendix will show that we \textit{can} extend Gaussian Oscillators, but the computational cost is unacceptably high.
Most Gaussian Oscillators are $O(n^2)$ in both time and space.
Oscillators with \textit{compact support} are the exception: they can be computed very efficiently ($O(1)$ with respect to $n$).
This suggests that compact support oscillators represent the future of Gaussian animations.

\subsection*{The general case}
\label{sub:the_general_case}

Let us treat the timetrace as a Gaussian process.
This means we have generated $n$ i.i.d. standard normal variates, $\xi^{(n)} \equiv \left\{\xi_1, \cdots, \xi_n\right\}$, and the lower-Cholesky decomposition $L^{(n)}$ of the covariance matrix $K^{(n)}$.
The first $n$ points are given by
\begin{equation}
  \alpha^{(n)} = L^{(n)} \xi^{(n)}.
\end{equation}

The covariance matrix $K^{(n + 1)}$ for the first $(n + 1)$ points can be written in block diagonal form:
\begin{equation}
  K^{(n + 1)} = \left[
    \begin{array}{cc}
      K^{(n)} & k^{(n)} \\
      k^{(n)\transpose} & 1
    \end{array}
    \right],
\end{equation}
where $K^{(n)}$ is the covariance matrix for the first $n$ points, and $k^{(n)}$ is a vector giving the covariance of the new point with each old point.
The Cholesky root can be similarly decomposed:
\begin{equation}
  L^{(n + 1)} = \left[
    \begin{array}{cc}
      L^{(n)} & 0 \\
      \ell^{(n)\transpose} & \gamma
    \end{array}
    \right],
\end{equation}
where $L^{(n)}$ is the Cholesky root of $K^{(n)}$, $\ell^{(n)}$ is an unknown vector, and $\gamma$ is an unknown scalar.

The $(n + 1)$st point is
\begin{equation}
  \alpha_{n + 1} = \left[
    \begin{array}{cc}
      \ell^{(n)\transpose} & \gamma
    \end{array}
  \right] \xi^{(n + 1)}.
  \label{eqn:next_point}
\end{equation}
At this point, we drop the $(n)$ and $(n + 1)$ superscripts for convenience.
The definition of the Cholesky root gives the following block matrix equation:
\begin{equation}
  \left[
    \begin{array}{cc}
      LL^\transpose & L\ell \\
      \ell^\transpose L^\transpose & \ell^\transpose \ell + \gamma^2
    \end{array}
  \right] = \left[
    \begin{array}{cc}
      K & k \\
      k^\transpose & 1
    \end{array}
  \right],
\end{equation}
which yields the vector equation
\begin{equation}
  L \ell = k
  \label{eqn:go_cov_old_new}
\end{equation}
and the scalar equation
\begin{equation}
  \ell^\transpose \ell + \gamma^2 = 1
\end{equation}
Since $L$ is lower-triangular and $k$ is known, Equation \ref{eqn:go_cov_old_new} can be trivially solved for $\ell$, and we can compute the $(n + 1)$st point.

However, the computational cost is catastrophic.
Imagine we leave the animation running, so that $n$ continues to increase.
The time to compute each new point by Equation \ref{eqn:go_cov_old_new} is $O(n^2)$.
The storage also increases as $O(n^2)$, since we require the entire matrix $L^{(n)}$.
This animation will consume all system resources at an ever-increasing pace, while delivering new points ever more slowly.

\subsection*{Compact Support}
\label{sub:compact_support}

To prevent this disaster, suppose the covariance matrix has compact support, so that the new value is correlated only with the previous $m$.
This means that $\left\{k_1, \cdots, k_{n - m} \right\}$ are all 0.
Equation \ref{eqn:go_cov_old_new} implies that $\left\{\ell_1, \cdots, \ell_{n - m} \right\}$ are also 0.

This yields enormous computational savings.
Since $L$ is lower-triangular, only the final $m \times m$ submatrix actually contributes to the calculation of $\ell$ (by Equation \ref{eqn:go_cov_old_new}).
Similarly, Equation \ref{eqn:next_point} only uses the $m$ most recent random variates, $\left\{\xi_{n - m + 1}, \cdots, \xi_n\right\}$; all the preceding ones can be discarded.
The important point to notice is that our computational requirements no longer grow without limit as the animation continues to run.

We can do still better.

Imagine now that the time interval between successive points is constant.
Then each row of $K$ (after the $m$th) is equivalent to the previous row, shifted by one column.
The same will be true for $L$.
Since each row has the same $m$ nonzero values, they need only be computed once.

We also only need to store $m$ random values, since the $n$th value can replace the $(n - m)$th (which is no longer needed).
The following R snippet\footnote{Note that this code is intended only to illustrate the concept; it is probably too inefficient for a real implementation.} illustrates how to generate an \texttt{n}-step timetrace, given a precomputed vector \texttt{ell} of length \texttt{m}:
<<compact_support_timetrace, echo=TRUE, eval=FALSE>>=
xi <- rnorm(m)
alpha <- c()
for (i in 1:n) {
  alpha <- c(alpha, xi %*% ell)
  xi <- c(tail(xi, -1), rnorm(1))
}
@
The code inside the \texttt{for}-loop can be repeated arbitrarily many times to extend the length of the Gaussian Oscillator.

To summarize: extending an $n$-point Gaussian Oscillator timetrace is generally $O(n^2)$ in both time and space, which is unacceptable.
But if the Gaussian Oscillator has compact support, both costs drop to $O(m^2)$ (where $m$ is small and constant).
And if we furthermore evaluate that oscillator only at constant intervals, the costs drop further to $O(m)$.

\bibliographystyle{plain}
\bibliography{biblio}
\end{document}

Ideas which need a home

(Looping vs. non-looping)
